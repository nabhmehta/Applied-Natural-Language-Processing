{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"Machine Translation Project","provenance":[{"file_id":"1d-oJtg35uV_3hjxb6frUAGIuE-7eafwc","timestamp":1609273847718}],"collapsed_sections":[],"toc_visible":true,"machine_shape":"hm"},"kernelspec":{"display_name":"Python 3","name":"python3"}},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XSt-thRlsXxo","outputId":"fc0562f6-30da-4e54-f7e4-a749de04dc1b"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"R5sJTwM6rC9v"},"source":["from pathlib import Path\n","folder=Path('/content/drive/MyDrive/NLP/pj2')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"soncOFrva9Jx"},"source":["data_path = '/content/drive/MyDrive/NLP/pj2/project.csv'"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"D07hJNs9f08J"},"source":["import pandas as pd\r\n","df = pd.read_table(data_path,sep=',')   #.iloc[:10000,:,]\r\n","#df.columns=['SRC','TRG','unkown']"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":206},"id":"lAN7xWcAf5k-","outputId":"3bb61064-261d-4d3b-9b5c-b83861c3e46f"},"source":["df.head()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>SRC</th>\n","      <th>TRG</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>Hi.</td>\n","      <td>嗨。</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>Hi.</td>\n","      <td>你好。</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>Run.</td>\n","      <td>你用跑的。</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>Wait!</td>\n","      <td>等等！</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>Wait!</td>\n","      <td>等一下！</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["     SRC    TRG\n","0    Hi.     嗨。\n","1    Hi.    你好。\n","2   Run.  你用跑的。\n","3  Wait!    等等！\n","4  Wait!   等一下！"]},"metadata":{"tags":[]},"execution_count":5}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wFdBj_Yyd4qQ","outputId":"0a873154-8e06-468d-c981-f1ebe74b59eb"},"source":["#!pip install torch"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (1.7.0+cu101)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torch) (1.18.5)\n","Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch) (0.16.0)\n","Requirement already satisfied: dataclasses in /usr/local/lib/python3.6/dist-packages (from torch) (0.8)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.6/dist-packages (from torch) (3.7.4.3)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"KtTENBhGbDfm"},"source":["!pip install torch==1.6.0 torchvision==0.7.0"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TtAI3EFPbWQn","outputId":"c02d6abb-6727-479e-c3de-90c7c032a86c"},"source":["!pip install torchtext==0.6.0"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: torchtext==0.6.0 in /usr/local/lib/python3.6/dist-packages (0.6.0)\n","Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from torchtext==0.6.0) (1.15.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torchtext==0.6.0) (1.18.5)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from torchtext==0.6.0) (4.41.1)\n","Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from torchtext==0.6.0) (2.23.0)\n","Requirement already satisfied: sentencepiece in /usr/local/lib/python3.6/dist-packages (from torchtext==0.6.0) (0.1.94)\n","Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (from torchtext==0.6.0) (1.7.0+cu101)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->torchtext==0.6.0) (2020.12.5)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->torchtext==0.6.0) (2.10)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->torchtext==0.6.0) (3.0.4)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->torchtext==0.6.0) (1.24.3)\n","Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch->torchtext==0.6.0) (0.16.0)\n","Requirement already satisfied: dataclasses in /usr/local/lib/python3.6/dist-packages (from torch->torchtext==0.6.0) (0.8)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.6/dist-packages (from torch->torchtext==0.6.0) (3.7.4.3)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"k3E-ZnaAfdRw"},"source":["import torch\r\n","import torch.nn as nn\r\n","import torch.optim as optim\r\n","import torch.nn.functional as F\r\n","\r\n","from torchtext.datasets import Multi30k\r\n","from torchtext.data import Field, BucketIterator\r\n","from torchtext import data\r\n","from torchtext import datasets\r\n","\r\n","import matplotlib.pyplot as plt\r\n","import matplotlib.ticker as ticker\r\n","\r\n","import spacy\r\n","import numpy as np\r\n","import pandas as pd\r\n","\r\n","import random\r\n","import math\r\n","import time"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"chv7RXzzW_M7","outputId":"d86590ad-e0a7-4f28-b1c6-6889bc04858c"},"source":["!pip install -U spacy"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Requirement already up-to-date: spacy in /usr/local/lib/python3.6/dist-packages (2.3.5)\n","Requirement already satisfied, skipping upgrade: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (2.23.0)\n","Requirement already satisfied, skipping upgrade: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy) (1.1.3)\n","Requirement already satisfied, skipping upgrade: numpy>=1.15.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (1.18.5)\n","Requirement already satisfied, skipping upgrade: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (0.8.0)\n","Requirement already satisfied, skipping upgrade: thinc<7.5.0,>=7.4.1 in /usr/local/lib/python3.6/dist-packages (from spacy) (7.4.5)\n","Requirement already satisfied, skipping upgrade: setuptools in /usr/local/lib/python3.6/dist-packages (from spacy) (50.3.2)\n","Requirement already satisfied, skipping upgrade: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.6/dist-packages (from spacy) (1.0.0)\n","Requirement already satisfied, skipping upgrade: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy) (1.0.5)\n","Requirement already satisfied, skipping upgrade: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (4.41.1)\n","Requirement already satisfied, skipping upgrade: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy) (2.0.5)\n","Requirement already satisfied, skipping upgrade: blis<0.8.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (0.4.1)\n","Requirement already satisfied, skipping upgrade: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy) (3.0.5)\n","Requirement already satisfied, skipping upgrade: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (1.0.5)\n","Requirement already satisfied, skipping upgrade: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (1.24.3)\n","Requirement already satisfied, skipping upgrade: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.0.4)\n","Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2020.12.5)\n","Requirement already satisfied, skipping upgrade: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.10)\n","Requirement already satisfied, skipping upgrade: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy) (3.1.1)\n","Requirement already satisfied, skipping upgrade: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy) (3.4.0)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KCMn5cPQWjhm","outputId":"7085a37a-aed3-4950-f551-198002453335"},"source":["!spacy download zh_core_web_lg"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: zh_core_web_lg==2.3.1 from https://github.com/explosion/spacy-models/releases/download/zh_core_web_lg-2.3.1/zh_core_web_lg-2.3.1.tar.gz#egg=zh_core_web_lg==2.3.1 in /usr/local/lib/python3.6/dist-packages (2.3.1)\n","Requirement already satisfied: jieba in /usr/local/lib/python3.6/dist-packages (from zh_core_web_lg==2.3.1) (0.42.1)\n","Requirement already satisfied: pkuseg>=0.0.22 in /usr/local/lib/python3.6/dist-packages (from zh_core_web_lg==2.3.1) (0.0.25)\n","Requirement already satisfied: spacy<2.4.0,>=2.3.0 in /usr/local/lib/python3.6/dist-packages (from zh_core_web_lg==2.3.1) (2.3.5)\n","Requirement already satisfied: numpy>=1.16.0 in /usr/local/lib/python3.6/dist-packages (from pkuseg>=0.0.22->zh_core_web_lg==2.3.1) (1.18.5)\n","Requirement already satisfied: cython in /usr/local/lib/python3.6/dist-packages (from pkuseg>=0.0.22->zh_core_web_lg==2.3.1) (0.29.21)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from spacy<2.4.0,>=2.3.0->zh_core_web_lg==2.3.1) (50.3.2)\n","Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy<2.4.0,>=2.3.0->zh_core_web_lg==2.3.1) (1.1.3)\n","Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy<2.4.0,>=2.3.0->zh_core_web_lg==2.3.1) (1.0.5)\n","Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy<2.4.0,>=2.3.0->zh_core_web_lg==2.3.1) (0.8.0)\n","Requirement already satisfied: blis<0.8.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy<2.4.0,>=2.3.0->zh_core_web_lg==2.3.1) (0.4.1)\n","Requirement already satisfied: thinc<7.5.0,>=7.4.1 in /usr/local/lib/python3.6/dist-packages (from spacy<2.4.0,>=2.3.0->zh_core_web_lg==2.3.1) (7.4.5)\n","Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.6/dist-packages (from spacy<2.4.0,>=2.3.0->zh_core_web_lg==2.3.1) (4.41.1)\n","Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.6/dist-packages (from spacy<2.4.0,>=2.3.0->zh_core_web_lg==2.3.1) (1.0.0)\n","Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.6/dist-packages (from spacy<2.4.0,>=2.3.0->zh_core_web_lg==2.3.1) (2.23.0)\n","Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy<2.4.0,>=2.3.0->zh_core_web_lg==2.3.1) (2.0.5)\n","Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy<2.4.0,>=2.3.0->zh_core_web_lg==2.3.1) (3.0.5)\n","Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy<2.4.0,>=2.3.0->zh_core_web_lg==2.3.1) (1.0.5)\n","Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy<2.4.0,>=2.3.0->zh_core_web_lg==2.3.1) (3.1.1)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy<2.4.0,>=2.3.0->zh_core_web_lg==2.3.1) (3.0.4)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy<2.4.0,>=2.3.0->zh_core_web_lg==2.3.1) (1.24.3)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy<2.4.0,>=2.3.0->zh_core_web_lg==2.3.1) (2.10)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy<2.4.0,>=2.3.0->zh_core_web_lg==2.3.1) (2020.12.5)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy<2.4.0,>=2.3.0->zh_core_web_lg==2.3.1) (3.4.0)\n","\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n","You can now load the model via spacy.load('zh_core_web_lg')\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"cMOlzkprWjkP","colab":{"base_uri":"https://localhost:8080/"},"outputId":"0bb7391a-3705-4930-8ae3-ac69bf3d14f0"},"source":["import zh_core_web_lg\r\n","spacy_zh = zh_core_web_lg.load() #Chinese"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Building prefix dict from the default dictionary ...\n","Loading model from cache /tmp/jieba.cache\n","Loading model cost 0.702 seconds.\n","Prefix dict has been built successfully.\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"cLCKpcFXWjme","outputId":"3c38373c-1bd7-41e1-a710-4ac6951c540b"},"source":["!spacy download en_core_web_lg"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: en_core_web_lg==2.3.1 from https://github.com/explosion/spacy-models/releases/download/en_core_web_lg-2.3.1/en_core_web_lg-2.3.1.tar.gz#egg=en_core_web_lg==2.3.1 in /usr/local/lib/python3.6/dist-packages (2.3.1)\n","Requirement already satisfied: spacy<2.4.0,>=2.3.0 in /usr/local/lib/python3.6/dist-packages (from en_core_web_lg==2.3.1) (2.3.5)\n","Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy<2.4.0,>=2.3.0->en_core_web_lg==2.3.1) (3.0.5)\n","Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.6/dist-packages (from spacy<2.4.0,>=2.3.0->en_core_web_lg==2.3.1) (4.41.1)\n","Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.6/dist-packages (from spacy<2.4.0,>=2.3.0->en_core_web_lg==2.3.1) (1.0.0)\n","Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.6/dist-packages (from spacy<2.4.0,>=2.3.0->en_core_web_lg==2.3.1) (1.18.5)\n","Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy<2.4.0,>=2.3.0->en_core_web_lg==2.3.1) (2.0.5)\n","Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy<2.4.0,>=2.3.0->en_core_web_lg==2.3.1) (1.1.3)\n","Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy<2.4.0,>=2.3.0->en_core_web_lg==2.3.1) (1.0.5)\n","Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.6/dist-packages (from spacy<2.4.0,>=2.3.0->en_core_web_lg==2.3.1) (2.23.0)\n","Requirement already satisfied: thinc<7.5.0,>=7.4.1 in /usr/local/lib/python3.6/dist-packages (from spacy<2.4.0,>=2.3.0->en_core_web_lg==2.3.1) (7.4.5)\n","Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy<2.4.0,>=2.3.0->en_core_web_lg==2.3.1) (1.0.5)\n","Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy<2.4.0,>=2.3.0->en_core_web_lg==2.3.1) (0.8.0)\n","Requirement already satisfied: blis<0.8.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy<2.4.0,>=2.3.0->en_core_web_lg==2.3.1) (0.4.1)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from spacy<2.4.0,>=2.3.0->en_core_web_lg==2.3.1) (50.3.2)\n","Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy<2.4.0,>=2.3.0->en_core_web_lg==2.3.1) (3.1.1)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy<2.4.0,>=2.3.0->en_core_web_lg==2.3.1) (1.24.3)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy<2.4.0,>=2.3.0->en_core_web_lg==2.3.1) (3.0.4)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy<2.4.0,>=2.3.0->en_core_web_lg==2.3.1) (2.10)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy<2.4.0,>=2.3.0->en_core_web_lg==2.3.1) (2020.12.5)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy<2.4.0,>=2.3.0->en_core_web_lg==2.3.1) (3.4.0)\n","\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n","You can now load the model via spacy.load('en_core_web_lg')\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"OQissxsdWjo1"},"source":["import en_core_web_lg\r\n","spacy_en = en_core_web_lg.load() #English"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"6_Qu4zZCWjrG"},"source":["SEED = 1234\r\n","\r\n","random.seed(SEED)\r\n","np.random.seed(SEED)\r\n","torch.manual_seed(SEED)\r\n","torch.cuda.manual_seed(SEED)\r\n","torch.backends.cudnn.deterministic = True"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"zeaq7-dRi7Kb"},"source":["# Tutorial 1"]},{"cell_type":"code","metadata":{"id":"ptiTO3hJMlDw"},"source":["def tokenize_zh(text):\r\n","    \"\"\"\r\n","    Tokenizes German text from a string into a list of strings (tokens) and reverses it\r\n","    \"\"\"\r\n","    return [tok.text for tok in spacy_zh.tokenizer(text)][::-1]\r\n","\r\n","def tokenize_en(text):\r\n","    \"\"\"\r\n","    Tokenizes English text from a string into a list of strings (tokens)\r\n","    \"\"\"\r\n","    return [tok.text for tok in spacy_en.tokenizer(text)]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"1LZ9_SKzMlD1"},"source":["SRC = Field(tokenize = tokenize_en, \r\n","            init_token = '<sos>', \r\n","            eos_token = '<eos>', \r\n","            lower = True)\r\n","\r\n","TRG = Field(tokenize = tokenize_zh, \r\n","            init_token = '<sos>', \r\n","            eos_token = '<eos>', \r\n","            lower = True)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FgtE2AVdmGdt","outputId":"9ec92418-b16d-4797-897a-324f100e90bb"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"XH6dkJWtX3LL"},"source":["import torchtext\r\n","\r\n","dataset = torchtext.data.TabularDataset(\r\n","    path='/content/drive/MyDrive/BUAN6342/Project2/project.csv',\r\n","    format='csv',\r\n","    skip_header=True,\r\n","    fields=[('src', SRC), ('trg', TRG)]\r\n",")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"0bCwDvOKX3NV"},"source":["train_data, valid_data, test_data = dataset.split(split_ratio=[0.6, 0.2, 0.2])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5_82g8_lMlD3","outputId":"572f7b9c-841b-427f-92f1-4accf8909f87"},"source":["print(f\"Number of training examples: {len(train_data.examples)}\")\r\n","print(f\"Number of validation examples: {len(valid_data.examples)}\")\r\n","print(f\"Number of testing examples: {len(test_data.examples)}\")"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Number of training examples: 14416\n","Number of validation examples: 4805\n","Number of testing examples: 4805\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"OhaPbLYpMlD4","outputId":"18dca910-23b7-4585-fd91-553ea2b8cd18"},"source":["print(vars(train_data.examples[0]))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["{'src': ['the', 'bicycle', 'by', 'the', 'door', 'is', 'mine', '.'], 'trg': ['。', '的', '我', '是', '自行車', '的', '門口', '在']}\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"J54FcpUBMlD4"},"source":["SRC.build_vocab(train_data, min_freq = 2)\r\n","TRG.build_vocab(train_data, min_freq = 2)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"i1foDdpSMlD5","outputId":"8bc15f06-5d4e-4c77-f4f7-6c815904e1d5"},"source":["print(f\"Unique tokens in source (zh) vocabulary: {len(SRC.vocab)}\")\r\n","print(f\"Unique tokens in target (en) vocabulary: {len(TRG.vocab)}\")"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Unique tokens in source (zh) vocabulary: 3087\n","Unique tokens in target (en) vocabulary: 4483\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"sngMZnxaYtH3"},"source":["device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"wMX0j8h_YtKO"},"source":["BATCH_SIZE = 128\r\n","\r\n","train_iterator, valid_iterator, test_iterator = BucketIterator.splits(\r\n","    (train_data, valid_data, test_data),\r\n","    sort=False, \r\n","    batch_size = BATCH_SIZE, \r\n","    device = device)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"gRZKhKPuYtMa"},"source":["\r\n","class Encoder(nn.Module):\r\n","    def __init__(self, input_dim, emb_dim, hid_dim, n_layers, dropout):\r\n","        super().__init__()\r\n","        \r\n","        self.hid_dim = hid_dim\r\n","        self.n_layers = n_layers\r\n","        \r\n","        self.embedding = nn.Embedding(input_dim, emb_dim)\r\n","        \r\n","        self.rnn = nn.LSTM(emb_dim, hid_dim, n_layers, dropout = dropout)\r\n","        \r\n","        self.dropout = nn.Dropout(dropout)\r\n","        \r\n","    def forward(self, src):\r\n","        \r\n","        #src = [src len, batch size]\r\n","        \r\n","        embedded = self.dropout(self.embedding(src))\r\n","        \r\n","        #embedded = [src len, batch size, emb dim]\r\n","        \r\n","        outputs, (hidden, cell) = self.rnn(embedded)\r\n","        \r\n","        #outputs = [src len, batch size, hid dim * n directions]\r\n","        #hidden = [n layers * n directions, batch size, hid dim]\r\n","        #cell = [n layers * n directions, batch size, hid dim]\r\n","        \r\n","        #outputs are always from the top hidden layer\r\n","        \r\n","        return hidden, cell"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"j133Ocj8aTDw"},"source":["class Decoder(nn.Module):\r\n","    def __init__(self, output_dim, emb_dim, hid_dim, n_layers, dropout):\r\n","        super().__init__()\r\n","        \r\n","        self.output_dim = output_dim\r\n","        self.hid_dim = hid_dim\r\n","        self.n_layers = n_layers\r\n","        \r\n","        self.embedding = nn.Embedding(output_dim, emb_dim)\r\n","        \r\n","        self.rnn = nn.LSTM(emb_dim, hid_dim, n_layers, dropout = dropout)\r\n","        \r\n","        self.fc_out = nn.Linear(hid_dim, output_dim)\r\n","        \r\n","        self.dropout = nn.Dropout(dropout)\r\n","        \r\n","    def forward(self, input, hidden, cell):\r\n","        \r\n","        #input = [batch size]\r\n","        #hidden = [n layers * n directions, batch size, hid dim]\r\n","        #cell = [n layers * n directions, batch size, hid dim]\r\n","        \r\n","        #n directions in the decoder will both always be 1, therefore:\r\n","        #hidden = [n layers, batch size, hid dim]\r\n","        #context = [n layers, batch size, hid dim]\r\n","        \r\n","        input = input.unsqueeze(0)\r\n","        \r\n","        #input = [1, batch size]\r\n","        \r\n","        embedded = self.dropout(self.embedding(input))\r\n","        \r\n","        #embedded = [1, batch size, emb dim]\r\n","                \r\n","        output, (hidden, cell) = self.rnn(embedded, (hidden, cell))\r\n","        \r\n","        #output = [seq len, batch size, hid dim * n directions]\r\n","        #hidden = [n layers * n directions, batch size, hid dim]\r\n","        #cell = [n layers * n directions, batch size, hid dim]\r\n","        \r\n","        #seq len and n directions will always be 1 in the decoder, therefore:\r\n","        #output = [1, batch size, hid dim]\r\n","        #hidden = [n layers, batch size, hid dim]\r\n","        #cell = [n layers, batch size, hid dim]\r\n","        \r\n","        prediction = self.fc_out(output.squeeze(0))\r\n","        \r\n","        #prediction = [batch size, output dim]\r\n","        \r\n","        return prediction, hidden, cell"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"9EJLKWJTaTGH"},"source":["class Seq2Seq(nn.Module):\r\n","    def __init__(self, encoder, decoder, device):\r\n","        super().__init__()\r\n","        \r\n","        self.encoder = encoder\r\n","        self.decoder = decoder\r\n","        self.device = device\r\n","        \r\n","        assert encoder.hid_dim == decoder.hid_dim, \\\r\n","            \"Hidden dimensions of encoder and decoder must be equal!\"\r\n","        assert encoder.n_layers == decoder.n_layers, \\\r\n","            \"Encoder and decoder must have equal number of layers!\"\r\n","        \r\n","    def forward(self, src, trg, teacher_forcing_ratio = 0.5):\r\n","        \r\n","        #src = [src len, batch size]\r\n","        #trg = [trg len, batch size]\r\n","        #teacher_forcing_ratio is probability to use teacher forcing\r\n","        #e.g. if teacher_forcing_ratio is 0.75 we use ground-truth inputs 75% of the time\r\n","        \r\n","        batch_size = trg.shape[1]\r\n","        trg_len = trg.shape[0]\r\n","        trg_vocab_size = self.decoder.output_dim\r\n","        \r\n","        #tensor to store decoder outputs\r\n","        outputs = torch.zeros(trg_len, batch_size, trg_vocab_size).to(self.device)\r\n","        \r\n","        #last hidden state of the encoder is used as the initial hidden state of the decoder\r\n","        hidden, cell = self.encoder(src)\r\n","        \r\n","        #first input to the decoder is the <sos> tokens\r\n","        input = trg[0,:]\r\n","        \r\n","        for t in range(1, trg_len):\r\n","            \r\n","            #insert input token embedding, previous hidden and previous cell states\r\n","            #receive output tensor (predictions) and new hidden and cell states\r\n","            output, hidden, cell = self.decoder(input, hidden, cell)\r\n","            \r\n","            #place predictions in a tensor holding predictions for each token\r\n","            outputs[t] = output\r\n","            \r\n","            #decide if we are going to use teacher forcing or not\r\n","            teacher_force = random.random() < teacher_forcing_ratio\r\n","            \r\n","            #get the highest predicted token from our predictions\r\n","            top1 = output.argmax(1) \r\n","            \r\n","            #if teacher forcing, use actual next token as next input\r\n","            #if not, use predicted token\r\n","            input = trg[t] if teacher_force else top1\r\n","        \r\n","        return outputs"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"FK7BOs_0aTIa"},"source":["\r\n","INPUT_DIM = len(SRC.vocab)\r\n","OUTPUT_DIM = len(TRG.vocab)\r\n","ENC_EMB_DIM = 256\r\n","DEC_EMB_DIM = 256\r\n","HID_DIM = 512\r\n","N_LAYERS = 2\r\n","ENC_DROPOUT = 0.5\r\n","DEC_DROPOUT = 0.5\r\n","\r\n","enc = Encoder(INPUT_DIM, ENC_EMB_DIM, HID_DIM, N_LAYERS, ENC_DROPOUT)\r\n","dec = Decoder(OUTPUT_DIM, DEC_EMB_DIM, HID_DIM, N_LAYERS, DEC_DROPOUT)\r\n","\r\n","model = Seq2Seq(enc, dec, device).to(device)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"GZc8iWXPaTK1","outputId":"7711311a-5808-41c5-bf84-af494b6b4164"},"source":["def init_weights(m):\r\n","    for name, param in m.named_parameters():\r\n","        nn.init.uniform_(param.data, -0.08, 0.08)\r\n","        \r\n","model.apply(init_weights)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["Seq2Seq(\n","  (encoder): Encoder(\n","    (embedding): Embedding(3087, 256)\n","    (rnn): LSTM(256, 512, num_layers=2, dropout=0.5)\n","    (dropout): Dropout(p=0.5, inplace=False)\n","  )\n","  (decoder): Decoder(\n","    (embedding): Embedding(4483, 256)\n","    (rnn): LSTM(256, 512, num_layers=2, dropout=0.5)\n","    (fc_out): Linear(in_features=512, out_features=4483, bias=True)\n","    (dropout): Dropout(p=0.5, inplace=False)\n","  )\n",")"]},"metadata":{"tags":[]},"execution_count":34}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6F3VoFSVaTNH","outputId":"92b3166f-af9e-4e16-c9e0-11303fdee580"},"source":["def count_parameters(model):\r\n","    return sum(p.numel() for p in model.parameters() if p.requires_grad)\r\n","\r\n","print(f'The model has {count_parameters(model):,} trainable parameters')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["The model has 11,594,115 trainable parameters\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"tqoNKrIyas_z"},"source":["\r\n","optimizer = optim.Adam(model.parameters())"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"TCNFTfSvafqA"},"source":["TRG_PAD_IDX = TRG.vocab.stoi[TRG.pad_token]\r\n","\r\n","criterion = nn.CrossEntropyLoss(ignore_index = TRG_PAD_IDX)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"AGJHnhnpafsh"},"source":["def train(model, iterator, optimizer, criterion, clip):\r\n","    \r\n","    model.train()\r\n","    \r\n","    epoch_loss = 0\r\n","    \r\n","    for i, batch in enumerate(iterator):\r\n","        \r\n","        src = batch.src\r\n","        trg = batch.trg\r\n","        \r\n","        optimizer.zero_grad()\r\n","        \r\n","        output = model(src, trg)\r\n","        \r\n","        #trg = [trg len, batch size]\r\n","        #output = [trg len, batch size, output dim]\r\n","        \r\n","        output_dim = output.shape[-1]\r\n","        \r\n","        output = output[1:].view(-1, output_dim)\r\n","        trg = trg[1:].view(-1)\r\n","        \r\n","        #trg = [(trg len - 1) * batch size]\r\n","        #output = [(trg len - 1) * batch size, output dim]\r\n","        \r\n","        loss = criterion(output, trg)\r\n","        \r\n","        loss.backward()\r\n","        \r\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\r\n","        \r\n","        optimizer.step()\r\n","        \r\n","        epoch_loss += loss.item()\r\n","        \r\n","    return epoch_loss / len(iterator)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"2k5gXS6Uafuq"},"source":["\r\n","def evaluate(model, iterator, criterion):\r\n","    \r\n","    model.eval()\r\n","    \r\n","    epoch_loss = 0\r\n","    \r\n","    with torch.no_grad():\r\n","    \r\n","        for i, batch in enumerate(iterator):\r\n","\r\n","            src = batch.src\r\n","            trg = batch.trg\r\n","\r\n","            output = model(src, trg, 0) #turn off teacher forcing\r\n","\r\n","            #trg = [trg len, batch size]\r\n","            #output = [trg len, batch size, output dim]\r\n","\r\n","            output_dim = output.shape[-1]\r\n","            \r\n","            output = output[1:].view(-1, output_dim)\r\n","            trg = trg[1:].view(-1)\r\n","\r\n","            #trg = [(trg len - 1) * batch size]\r\n","            #output = [(trg len - 1) * batch size, output dim]\r\n","\r\n","            loss = criterion(output, trg)\r\n","            \r\n","            epoch_loss += loss.item()\r\n","        \r\n","    return epoch_loss / len(iterator)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"IO2Kjfb1afxI"},"source":["def epoch_time(start_time, end_time):\r\n","    elapsed_time = end_time - start_time\r\n","    elapsed_mins = int(elapsed_time / 60)\r\n","    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\r\n","    return elapsed_mins, elapsed_secs"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"quq8Icccafzu","outputId":"f17b46a9-340c-4620-a916-dd4aa3c6cc56"},"source":["\r\n","N_EPOCHS = 10\r\n","CLIP = 1\r\n","\r\n","best_valid_loss = float('inf')\r\n","\r\n","for epoch in range(N_EPOCHS):\r\n","    \r\n","    start_time = time.time()\r\n","    \r\n","    train_loss = train(model, train_iterator, optimizer, criterion, CLIP)\r\n","    valid_loss = evaluate(model, valid_iterator, criterion)\r\n","    \r\n","    end_time = time.time()\r\n","    \r\n","    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\r\n","    \r\n","    if valid_loss < best_valid_loss:\r\n","        best_valid_loss = valid_loss\r\n","        torch.save(model.state_dict(), 'tut1-model.pt')\r\n","    \r\n","    print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\r\n","    print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}')\r\n","    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. PPL: {math.exp(valid_loss):7.3f}')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Epoch: 01 | Time: 6m 46s\n","\tTrain Loss: 5.012 | Train PPL: 150.152\n","\t Val. Loss: 4.568 |  Val. PPL:  96.350\n","Epoch: 02 | Time: 6m 40s\n","\tTrain Loss: 4.677 | Train PPL: 107.469\n","\t Val. Loss: 4.553 |  Val. PPL:  94.915\n","Epoch: 03 | Time: 6m 40s\n","\tTrain Loss: 4.563 | Train PPL:  95.909\n","\t Val. Loss: 4.520 |  Val. PPL:  91.816\n","Epoch: 04 | Time: 6m 43s\n","\tTrain Loss: 4.464 | Train PPL:  86.859\n","\t Val. Loss: 4.444 |  Val. PPL:  85.085\n","Epoch: 05 | Time: 6m 37s\n","\tTrain Loss: 4.345 | Train PPL:  77.062\n","\t Val. Loss: 4.380 |  Val. PPL:  79.821\n","Epoch: 06 | Time: 6m 42s\n","\tTrain Loss: 4.174 | Train PPL:  64.950\n","\t Val. Loss: 4.282 |  Val. PPL:  72.418\n","Epoch: 07 | Time: 6m 41s\n","\tTrain Loss: 4.041 | Train PPL:  56.857\n","\t Val. Loss: 4.201 |  Val. PPL:  66.767\n","Epoch: 08 | Time: 6m 45s\n","\tTrain Loss: 3.872 | Train PPL:  48.027\n","\t Val. Loss: 4.148 |  Val. PPL:  63.290\n","Epoch: 09 | Time: 6m 46s\n","\tTrain Loss: 3.731 | Train PPL:  41.722\n","\t Val. Loss: 4.075 |  Val. PPL:  58.856\n","Epoch: 10 | Time: 6m 45s\n","\tTrain Loss: 3.585 | Train PPL:  36.066\n","\t Val. Loss: 4.045 |  Val. PPL:  57.101\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"n5If70MXi9_c"},"source":["## Performance: Test Loss=4.050"]},{"cell_type":"code","metadata":{"id":"z6flFvMoaf2D","colab":{"base_uri":"https://localhost:8080/"},"outputId":"1cc6f530-337c-4a79-ff03-916566e07e3f"},"source":["model.load_state_dict(torch.load('tut1-model.pt'))\r\n","\r\n","test_loss = evaluate(model, test_iterator, criterion)\r\n","\r\n","print(f'| Test Loss: {test_loss:.3f} | Test PPL: {math.exp(test_loss):7.3f} |')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["| Test Loss: 4.050 | Test PPL:  57.425 |\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"mZTrpT0j2eVT"},"source":["def translate_sentence(sentence, src_field, trg_field, model, device, max_len = 50):\r\n","    \r\n","    model.eval()\r\n","        \r\n","    if isinstance(sentence, str):\r\n","        nlp = zh_core_web_lg.load()\r\n","        tokens = [token.text.lower() for token in nlp(sentence)]\r\n","    else:\r\n","        tokens = [token.lower() for token in sentence]\r\n","\r\n","    tokens = [src_field.init_token] + tokens + [src_field.eos_token]\r\n","        \r\n","    src_indexes = [src_field.vocab.stoi[token] for token in tokens]\r\n","\r\n","    src_tensor = torch.LongTensor(src_indexes).unsqueeze(0).to(device)\r\n","    \r\n","    src_mask = model.make_src_mask(src_tensor)\r\n","    \r\n","    with torch.no_grad():\r\n","        enc_src = model.encoder(src_tensor, src_mask)\r\n","\r\n","    trg_indexes = [trg_field.vocab.stoi[trg_field.init_token]]\r\n","\r\n","    for i in range(max_len):\r\n","\r\n","        trg_tensor = torch.LongTensor(trg_indexes).unsqueeze(0).to(device)\r\n","\r\n","        trg_mask = model.make_trg_mask(trg_tensor)\r\n","        \r\n","        with torch.no_grad():\r\n","            output, attention = model.decoder(trg_tensor, enc_src, trg_mask, src_mask)\r\n","        \r\n","        pred_token = output.argmax(2)[:,-1].item()\r\n","        \r\n","        trg_indexes.append(pred_token)\r\n","\r\n","        if pred_token == trg_field.vocab.stoi[trg_field.eos_token]:\r\n","            break\r\n","    \r\n","    trg_tokens = [trg_field.vocab.itos[i] for i in trg_indexes]\r\n","    \r\n","    return trg_tokens[1:], attention"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"jReXAc1s2fhQ"},"source":["def display_attention(sentence, translation, attention, n_heads = 8, n_rows = 4, n_cols = 2):\r\n","    \r\n","    assert n_rows * n_cols == n_heads\r\n","    \r\n","    fig = plt.figure(figsize=(15,25))\r\n","    \r\n","    for i in range(n_heads):\r\n","        \r\n","        ax = fig.add_subplot(n_rows, n_cols, i+1)\r\n","        \r\n","        _attention = attention.squeeze(0)[i].cpu().detach().numpy()\r\n","\r\n","        cax = ax.matshow(_attention, cmap='bone')\r\n","\r\n","        ax.tick_params(labelsize=12)\r\n","        ax.set_xticklabels(['']+['<sos>']+[t.lower() for t in sentence]+['<eos>'], \r\n","                           rotation=45)\r\n","        ax.set_yticklabels(['']+translation)\r\n","\r\n","        ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\r\n","        ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\r\n","\r\n","    plt.show()\r\n","    plt.close()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6q3tvjgF2fkO","outputId":"e4ee8390-5a25-4da4-8ef1-80990219cc78"},"source":["example_idx = 8\r\n","\r\n","src = vars(train_data.examples[example_idx])['src']\r\n","trg = vars(train_data.examples[example_idx])['trg']\r\n","\r\n","print(f'src = {src}')\r\n","print(f'trg = {trg}')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["src = ['i', 'ca', \"n't\", 'sleep', 'well', '.']\n","trg = ['。', '好', '不', '睡', '我']\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"OXnPJmKxi5yq"},"source":["# Tutorial 2"]},{"cell_type":"code","metadata":{"id":"Uhl1tWGBoGdb"},"source":["def tokenize_zh(text):\r\n","    \"\"\"\r\n","    Tokenizes Chinese text from a string into a list of strings (tokens) and reverses it\r\n","    \"\"\"\r\n","    return [tok.text for tok in spacy_zh.tokenizer(text)][::-1]\r\n","\r\n","def tokenize_en(text):\r\n","    \"\"\"\r\n","    Tokenizes English text from a string into a list of strings (tokens)\r\n","    \"\"\"\r\n","    return [tok.text for tok in spacy_en.tokenizer(text)]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"qFyF2iS4oGfz"},"source":["SRC = Field(tokenize = tokenize_en, \r\n","            init_token = '<sos>', \r\n","            eos_token = '<eos>', \r\n","            lower = True)\r\n","\r\n","TRG = Field(tokenize = tokenize_zh, \r\n","            init_token = '<sos>', \r\n","            eos_token = '<eos>', \r\n","            lower = True)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"N6y-lh9oOPfs","outputId":"adef1f6e-beaa-4be9-9d56-bc860252b156"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"jrj4h65fogH8"},"source":["import torchtext\r\n","\r\n","dataset = torchtext.data.TabularDataset(\r\n","    path='/content/drive/MyDrive/BUAN6342/Project2/project.csv',\r\n","    format='csv',\r\n","    skip_header=True,\r\n","    fields=[('src', SRC), ('trg', TRG)]\r\n",")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"lDhimtJPogKC"},"source":["train_data, valid_data, test_data = dataset.split(split_ratio=[0.6, 0.2, 0.2])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"DU-weh31ogMa","colab":{"base_uri":"https://localhost:8080/"},"outputId":"9ab40624-b1a3-4514-a77a-c8381212a8ae"},"source":["print(vars(train_data.examples[0]))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["{'src': ['the', 'bicycle', 'by', 'the', 'door', 'is', 'mine', '.'], 'trg': ['。', '的', '我', '是', '自行車', '的', '門口', '在']}\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"yaRI58auogOp"},"source":["\r\n","SRC.build_vocab(train_data, min_freq = 2)\r\n","TRG.build_vocab(train_data, min_freq = 2)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"j-KCfnxroq5O"},"source":["device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"n7gVQ5SToq7c"},"source":["BATCH_SIZE = 128\r\n","\r\n","train_iterator, valid_iterator, test_iterator = BucketIterator.splits(\r\n","    (train_data, valid_data, test_data),\r\n","    sort=False, \r\n","    batch_size = BATCH_SIZE, \r\n","    device = device)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"8SOc9-uSoq90"},"source":["\r\n","class Encoder(nn.Module):\r\n","    def __init__(self, input_dim, emb_dim, hid_dim, dropout):\r\n","        super().__init__()\r\n","\r\n","        self.hid_dim = hid_dim\r\n","        \r\n","        self.embedding = nn.Embedding(input_dim, emb_dim) #no dropout as only one layer!\r\n","        \r\n","        self.rnn = nn.GRU(emb_dim, hid_dim)\r\n","        \r\n","        self.dropout = nn.Dropout(dropout)\r\n","        \r\n","    def forward(self, src):\r\n","        \r\n","        #src = [src len, batch size]\r\n","        \r\n","        embedded = self.dropout(self.embedding(src))\r\n","        \r\n","        #embedded = [src len, batch size, emb dim]\r\n","        \r\n","        outputs, hidden = self.rnn(embedded) #no cell state!\r\n","        \r\n","        #outputs = [src len, batch size, hid dim * n directions]\r\n","        #hidden = [n layers * n directions, batch size, hid dim]\r\n","        \r\n","        #outputs are always from the top hidden layer\r\n","        \r\n","        return hidden"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"vfKlROs-o0pK"},"source":["class Decoder(nn.Module):\r\n","    def __init__(self, output_dim, emb_dim, hid_dim, dropout):\r\n","        super().__init__()\r\n","\r\n","        self.hid_dim = hid_dim\r\n","        self.output_dim = output_dim\r\n","        \r\n","        self.embedding = nn.Embedding(output_dim, emb_dim)\r\n","        \r\n","        self.rnn = nn.GRU(emb_dim + hid_dim, hid_dim)\r\n","        \r\n","        self.fc_out = nn.Linear(emb_dim + hid_dim * 2, output_dim)\r\n","        \r\n","        self.dropout = nn.Dropout(dropout)\r\n","        \r\n","    def forward(self, input, hidden, context):\r\n","        \r\n","        #input = [batch size]\r\n","        #hidden = [n layers * n directions, batch size, hid dim]\r\n","        #context = [n layers * n directions, batch size, hid dim]\r\n","        \r\n","        #n layers and n directions in the decoder will both always be 1, therefore:\r\n","        #hidden = [1, batch size, hid dim]\r\n","        #context = [1, batch size, hid dim]\r\n","        \r\n","        input = input.unsqueeze(0)\r\n","        \r\n","        #input = [1, batch size]\r\n","        \r\n","        embedded = self.dropout(self.embedding(input))\r\n","        \r\n","        #embedded = [1, batch size, emb dim]\r\n","                \r\n","        emb_con = torch.cat((embedded, context), dim = 2)\r\n","            \r\n","        #emb_con = [1, batch size, emb dim + hid dim]\r\n","            \r\n","        output, hidden = self.rnn(emb_con, hidden)\r\n","        \r\n","        #output = [seq len, batch size, hid dim * n directions]\r\n","        #hidden = [n layers * n directions, batch size, hid dim]\r\n","        \r\n","        #seq len, n layers and n directions will always be 1 in the decoder, therefore:\r\n","        #output = [1, batch size, hid dim]\r\n","        #hidden = [1, batch size, hid dim]\r\n","        \r\n","        output = torch.cat((embedded.squeeze(0), hidden.squeeze(0), context.squeeze(0)), \r\n","                           dim = 1)\r\n","        \r\n","        #output = [batch size, emb dim + hid dim * 2]\r\n","        \r\n","        prediction = self.fc_out(output)\r\n","        \r\n","        #prediction = [batch size, output dim]\r\n","        \r\n","        return prediction, hidden"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"MuRRBxc3o0rb"},"source":["class Seq2Seq(nn.Module):\r\n","    def __init__(self, encoder, decoder, device):\r\n","        super().__init__()\r\n","        \r\n","        self.encoder = encoder\r\n","        self.decoder = decoder\r\n","        self.device = device\r\n","        \r\n","        assert encoder.hid_dim == decoder.hid_dim, \\\r\n","            \"Hidden dimensions of encoder and decoder must be equal!\"\r\n","        \r\n","    def forward(self, src, trg, teacher_forcing_ratio = 0.5):\r\n","        \r\n","        #src = [src len, batch size]\r\n","        #trg = [trg len, batch size]\r\n","        #teacher_forcing_ratio is probability to use teacher forcing\r\n","        #e.g. if teacher_forcing_ratio is 0.75 we use ground-truth inputs 75% of the time\r\n","        \r\n","        batch_size = trg.shape[1]\r\n","        trg_len = trg.shape[0]\r\n","        trg_vocab_size = self.decoder.output_dim\r\n","        \r\n","        #tensor to store decoder outputs\r\n","        outputs = torch.zeros(trg_len, batch_size, trg_vocab_size).to(self.device)\r\n","        \r\n","        #last hidden state of the encoder is the context\r\n","        context = self.encoder(src)\r\n","        \r\n","        #context also used as the initial hidden state of the decoder\r\n","        hidden = context\r\n","        \r\n","        #first input to the decoder is the <sos> tokens\r\n","        input = trg[0,:]\r\n","        \r\n","        for t in range(1, trg_len):\r\n","            \r\n","            #insert input token embedding, previous hidden state and the context state\r\n","            #receive output tensor (predictions) and new hidden state\r\n","            output, hidden = self.decoder(input, hidden, context)\r\n","            \r\n","            #place predictions in a tensor holding predictions for each token\r\n","            outputs[t] = output\r\n","            \r\n","            #decide if we are going to use teacher forcing or not\r\n","            teacher_force = random.random() < teacher_forcing_ratio\r\n","            \r\n","            #get the highest predicted token from our predictions\r\n","            top1 = output.argmax(1) \r\n","            \r\n","            #if teacher forcing, use actual next token as next input\r\n","            #if not, use predicted token\r\n","            input = trg[t] if teacher_force else top1\r\n","\r\n","        return outputs"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"70mSU9RRo0tx"},"source":["\r\n","INPUT_DIM = len(SRC.vocab)\r\n","OUTPUT_DIM = len(TRG.vocab)\r\n","ENC_EMB_DIM = 256\r\n","DEC_EMB_DIM = 256\r\n","HID_DIM = 512\r\n","ENC_DROPOUT = 0.5\r\n","DEC_DROPOUT = 0.5\r\n","\r\n","enc = Encoder(INPUT_DIM, ENC_EMB_DIM, HID_DIM, ENC_DROPOUT)\r\n","dec = Decoder(OUTPUT_DIM, DEC_EMB_DIM, HID_DIM, DEC_DROPOUT)\r\n","\r\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\r\n","\r\n","model = Seq2Seq(enc, dec, device).to(device)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Mu3L8x3Lo5GC","colab":{"base_uri":"https://localhost:8080/"},"outputId":"2a36eafb-92f7-45a5-b288-1e3edd98603d"},"source":["def init_weights(m):\r\n","    for name, param in m.named_parameters():\r\n","        nn.init.normal_(param.data, mean=0, std=0.01)\r\n","        \r\n","model.apply(init_weights)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["Seq2Seq(\n","  (encoder): Encoder(\n","    (embedding): Embedding(3087, 256)\n","    (rnn): GRU(256, 512)\n","    (dropout): Dropout(p=0.5, inplace=False)\n","  )\n","  (decoder): Decoder(\n","    (embedding): Embedding(4483, 256)\n","    (rnn): GRU(768, 512)\n","    (fc_out): Linear(in_features=1280, out_features=4483, bias=True)\n","    (dropout): Dropout(p=0.5, inplace=False)\n","  )\n",")"]},"metadata":{"tags":[]},"execution_count":22}]},{"cell_type":"code","metadata":{"id":"90WtX7czo5IP","colab":{"base_uri":"https://localhost:8080/"},"outputId":"31ae30be-da95-487d-cecd-45ce58aed2cb"},"source":["def count_parameters(model):\r\n","    return sum(p.numel() for p in model.parameters() if p.requires_grad)\r\n","\r\n","print(f'The model has {count_parameters(model):,} trainable parameters')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["The model has 10,832,515 trainable parameters\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"KswMiGNdo5Kw"},"source":["optimizer = optim.Adam(model.parameters())"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"QAidKljyo5NS"},"source":["\r\n","TRG_PAD_IDX = TRG.vocab.stoi[TRG.pad_token]\r\n","\r\n","criterion = nn.CrossEntropyLoss(ignore_index = TRG_PAD_IDX)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"U6EACVqWo5Pj"},"source":["def train(model, iterator, optimizer, criterion, clip):\r\n","    \r\n","    model.train()\r\n","    \r\n","    epoch_loss = 0\r\n","    \r\n","    for i, batch in enumerate(iterator):\r\n","        \r\n","        src = batch.src\r\n","        trg = batch.trg\r\n","        \r\n","        optimizer.zero_grad()\r\n","        \r\n","        output = model(src, trg)\r\n","        \r\n","        #trg = [trg len, batch size]\r\n","        #output = [trg len, batch size, output dim]\r\n","        \r\n","        output_dim = output.shape[-1]\r\n","        \r\n","        output = output[1:].view(-1, output_dim)\r\n","        trg = trg[1:].view(-1)\r\n","        \r\n","        #trg = [(trg len - 1) * batch size]\r\n","        #output = [(trg len - 1) * batch size, output dim]\r\n","        \r\n","        loss = criterion(output, trg)\r\n","        \r\n","        loss.backward()\r\n","        \r\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\r\n","        \r\n","        optimizer.step()\r\n","        \r\n","        epoch_loss += loss.item()\r\n","        \r\n","    return epoch_loss / len(iterator)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"8LOx-L3PpIFd"},"source":["def evaluate(model, iterator, criterion):\r\n","    \r\n","    model.eval()\r\n","    \r\n","    epoch_loss = 0\r\n","    \r\n","    with torch.no_grad():\r\n","    \r\n","        for i, batch in enumerate(iterator):\r\n","\r\n","            src = batch.src\r\n","            trg = batch.trg\r\n","\r\n","            output = model(src, trg, 0) #turn off teacher forcing\r\n","\r\n","            #trg = [trg len, batch size]\r\n","            #output = [trg len, batch size, output dim]\r\n","\r\n","            output_dim = output.shape[-1]\r\n","            \r\n","            output = output[1:].view(-1, output_dim)\r\n","            trg = trg[1:].view(-1)\r\n","\r\n","            #trg = [(trg len - 1) * batch size]\r\n","            #output = [(trg len - 1) * batch size, output dim]\r\n","\r\n","            loss = criterion(output, trg)\r\n","\r\n","            epoch_loss += loss.item()\r\n","        \r\n","    return epoch_loss / len(iterator)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"eRrKmfCrpIHk"},"source":["def epoch_time(start_time, end_time):\r\n","    elapsed_time = end_time - start_time\r\n","    elapsed_mins = int(elapsed_time / 60)\r\n","    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\r\n","    return elapsed_mins, elapsed_secs"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"XlvUk9NwpIKC","colab":{"base_uri":"https://localhost:8080/"},"outputId":"ed98b6eb-9907-4554-b147-b8bae5c76ed4"},"source":["\r\n","N_EPOCHS = 5\r\n","CLIP = 1\r\n","\r\n","best_valid_loss = float('inf')\r\n","\r\n","for epoch in range(N_EPOCHS):\r\n","    \r\n","    start_time = time.time()\r\n","    \r\n","    train_loss = train(model, train_iterator, optimizer, criterion, CLIP)\r\n","    valid_loss = evaluate(mProjodel, valid_iterator, criterion)\r\n","    \r\n","    end_time = time.time()\r\n","    \r\n","    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\r\n","    \r\n","    if valid_loss < best_valid_loss:\r\n","        best_valid_loss = valid_loss\r\n","        torch.save(model.state_dict(), 'tut2-model.pt')\r\n","    \r\n","    print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\r\n","    print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}')\r\n","    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. PPL: {math.exp(valid_loss):7.3f}')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Epoch: 01 | Time: 6m 20s\n","\tTrain Loss: 4.598 | Train PPL:  99.260\n","\t Val. Loss: 4.523 |  Val. PPL:  92.102\n","Epoch: 02 | Time: 6m 20s\n","\tTrain Loss: 4.388 | Train PPL:  80.512\n","\t Val. Loss: 4.386 |  Val. PPL:  80.309\n","Epoch: 03 | Time: 6m 16s\n","\tTrain Loss: 4.204 | Train PPL:  66.957\n","\t Val. Loss: 4.303 |  Val. PPL:  73.895\n","Epoch: 04 | Time: 6m 18s\n","\tTrain Loss: 4.055 | Train PPL:  57.711\n","\t Val. Loss: 4.269 |  Val. PPL:  71.464\n","Epoch: 05 | Time: 6m 19s\n","\tTrain Loss: 3.916 | Train PPL:  50.206\n","\t Val. Loss: 4.196 |  Val. PPL:  66.422\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"QhgWHxiNjCcm"},"source":["## Performance: Test Loss=4.200"]},{"cell_type":"code","metadata":{"id":"eUWp8XdxpMvS","colab":{"base_uri":"https://localhost:8080/"},"outputId":"cda62bbe-fd82-435c-9b7d-c186e2ecd011"},"source":["model.load_state_dict(torch.load('tut2-model.pt'))\r\n","\r\n","test_loss = evaluate(model, test_iterator, criterion)\r\n","\r\n","print(f'| Test Loss: {test_loss:.3f} | Test PPL: {math.exp(test_loss):7.3f} |')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["| Test Loss: 4.200 | Test PPL:  66.707 |\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"wctd-ZsWi4eo"},"source":["# Tutorial 3"]},{"cell_type":"code","metadata":{"id":"6PHJRgyp8X0R"},"source":["def tokenize_zh(text):\r\n","    \"\"\"\r\n","    Tokenizes German text from a string into a list of strings (tokens) and reverses it\r\n","    \"\"\"\r\n","    return [tok.text for tok in spacy_zh.tokenizer(text)][::-1]\r\n","\r\n","def tokenize_en(text):\r\n","    \"\"\"\r\n","    Tokenizes English text from a string into a list of strings (tokens)\r\n","    \"\"\"\r\n","    return [tok.text for tok in spacy_en.tokenizer(text)]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"lQlnrQer8Xxi"},"source":["TRG = Field(tokenize = tokenize_zh, \r\n","            init_token = '<sos>', \r\n","            eos_token = '<eos>', \r\n","            lower = True)\r\n","\r\n","SRC = Field(tokenize = tokenize_en, \r\n","            init_token = '<sos>', \r\n","            eos_token = '<eos>', \r\n","            lower = True)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"hSsjFT1h8Xu1"},"source":["import torchtext\r\n","\r\n","dataset = torchtext.data.TabularDataset(\r\n","    path='/content/cmn-eng/project.csv',\r\n","    format='csv',\r\n","    skip_header=True,\r\n","    fields=[('src', SRC), ('trg', TRG)]\r\n",")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"n4pv3SME8Xr_"},"source":["train_data, valid_data, test_data = dataset.split(split_ratio=[0.6, 0.2, 0.2])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"s2DC4KK78Xo5","outputId":"175774f4-60ad-43d6-8385-5fa6bd127523"},"source":["print(f\"Number of training examples: {len(train_data.examples)}\")\r\n","print(f\"Number of validation examples: {len(valid_data.examples)}\")\r\n","print(f\"Number of testing examples: {len(test_data.examples)}\")"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Number of training examples: 14416\n","Number of validation examples: 4805\n","Number of testing examples: 4805\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"f4to9GJQeoEQ"},"source":["SRC.build_vocab(train_data, min_freq = 2)\n","TRG.build_vocab(train_data, min_freq = 2)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"VXMtmzWteoEQ"},"source":["Define the device."]},{"cell_type":"code","metadata":{"id":"w4pbCEBFeoER"},"source":["device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"y0VJej7yeoER"},"source":["Create the iterators."]},{"cell_type":"code","metadata":{"id":"kT1WzxYreoER"},"source":["BATCH_SIZE = 128\n","\n","train_iterator, valid_iterator, test_iterator = BucketIterator.splits(\n","    (train_data, valid_data, test_data), \n","    batch_size = BATCH_SIZE,\n","    sort = False,\n","    device = device)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"cylrOed5eoES"},"source":["class Encoder(nn.Module):\n","    def __init__(self, input_dim, emb_dim, enc_hid_dim, dec_hid_dim, dropout):\n","        super().__init__()\n","        \n","        self.embedding = nn.Embedding(input_dim, emb_dim)\n","        \n","        self.rnn = nn.GRU(emb_dim, enc_hid_dim, bidirectional = True)\n","        \n","        self.fc = nn.Linear(enc_hid_dim * 2, dec_hid_dim)\n","        \n","        self.dropout = nn.Dropout(dropout)\n","        \n","    def forward(self, src):\n","        \n","        #src = [src len, batch size]\n","        \n","        embedded = self.dropout(self.embedding(src))\n","        \n","        #embedded = [src len, batch size, emb dim]\n","        \n","        outputs, hidden = self.rnn(embedded)\n","                \n","        #outputs = [src len, batch size, hid dim * num directions]\n","        #hidden = [n layers * num directions, batch size, hid dim]\n","        \n","        #hidden is stacked [forward_1, backward_1, forward_2, backward_2, ...]\n","        #outputs are always from the last layer\n","        \n","        #hidden [-2, :, : ] is the last of the forwards RNN \n","        #hidden [-1, :, : ] is the last of the backwards RNN\n","        \n","        #initial decoder hidden is final hidden state of the forwards and backwards \n","        #  encoder RNNs fed through a linear layer\n","        hidden = torch.tanh(self.fc(torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim = 1)))\n","        \n","        #outputs = [src len, batch size, enc hid dim * 2]\n","        #hidden = [batch size, dec hid dim]\n","        \n","        return outputs, hidden"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"nUWKL6sA8XgJ"},"source":["class Attention(nn.Module):\r\n","    def __init__(self, enc_hid_dim, dec_hid_dim):\r\n","        super().__init__()\r\n","        \r\n","        self.attn = nn.Linear((enc_hid_dim * 2) + dec_hid_dim, dec_hid_dim)\r\n","        self.v = nn.Linear(dec_hid_dim, 1, bias = False)\r\n","        \r\n","    def forward(self, hidden, encoder_outputs):\r\n","        \r\n","        #hidden = [batch size, dec hid dim]\r\n","        #encoder_outputs = [src len, batch size, enc hid dim * 2]\r\n","        \r\n","        batch_size = encoder_outputs.shape[1]\r\n","        src_len = encoder_outputs.shape[0]\r\n","        \r\n","        #repeat decoder hidden state src_len times\r\n","        hidden = hidden.unsqueeze(1).repeat(1, src_len, 1)\r\n","        \r\n","        encoder_outputs = encoder_outputs.permute(1, 0, 2)\r\n","        \r\n","        #hidden = [batch size, src len, dec hid dim]\r\n","        #encoder_outputs = [batch size, src len, enc hid dim * 2]\r\n","        \r\n","        energy = torch.tanh(self.attn(torch.cat((hidden, encoder_outputs), dim = 2))) \r\n","        \r\n","        #energy = [batch size, src len, dec hid dim]\r\n","\r\n","        attention = self.v(energy).squeeze(2)\r\n","        \r\n","        #attention= [batch size, src len]\r\n","        \r\n","        return F.softmax(attention, dim=1)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"kqzx9rHT8XdN"},"source":["class Decoder(nn.Module):\r\n","    def __init__(self, output_dim, emb_dim, enc_hid_dim, dec_hid_dim, dropout, attention):\r\n","        super().__init__()\r\n","\r\n","        self.output_dim = output_dim\r\n","        self.attention = attention\r\n","        \r\n","        self.embedding = nn.Embedding(output_dim, emb_dim)\r\n","        \r\n","        self.rnn = nn.GRU((enc_hid_dim * 2) + emb_dim, dec_hid_dim)\r\n","        \r\n","        self.fc_out = nn.Linear((enc_hid_dim * 2) + dec_hid_dim + emb_dim, output_dim)\r\n","        \r\n","        self.dropout = nn.Dropout(dropout)\r\n","        \r\n","    def forward(self, input, hidden, encoder_outputs):\r\n","             \r\n","        #input = [batch size]\r\n","        #hidden = [batch size, dec hid dim]\r\n","        #encoder_outputs = [src len, batch size, enc hid dim * 2]\r\n","        \r\n","        input = input.unsqueeze(0)\r\n","        \r\n","        #input = [1, batch size]\r\n","        \r\n","        embedded = self.dropout(self.embedding(input))\r\n","        \r\n","        #embedded = [1, batch size, emb dim]\r\n","        \r\n","        a = self.attention(hidden, encoder_outputs)\r\n","                \r\n","        #a = [batch size, src len]\r\n","        \r\n","        a = a.unsqueeze(1)\r\n","        \r\n","        #a = [batch size, 1, src len]\r\n","        \r\n","        encoder_outputs = encoder_outputs.permute(1, 0, 2)\r\n","        \r\n","        #encoder_outputs = [batch size, src len, enc hid dim * 2]\r\n","        \r\n","        weighted = torch.bmm(a, encoder_outputs)\r\n","        \r\n","        #weighted = [batch size, 1, enc hid dim * 2]\r\n","        \r\n","        weighted = weighted.permute(1, 0, 2)\r\n","        \r\n","        #weighted = [1, batch size, enc hid dim * 2]\r\n","        \r\n","        rnn_input = torch.cat((embedded, weighted), dim = 2)\r\n","        \r\n","        #rnn_input = [1, batch size, (enc hid dim * 2) + emb dim]\r\n","            \r\n","        output, hidden = self.rnn(rnn_input, hidden.unsqueeze(0))\r\n","        \r\n","        #output = [seq len, batch size, dec hid dim * n directions]\r\n","        #hidden = [n layers * n directions, batch size, dec hid dim]\r\n","        \r\n","        #seq len, n layers and n directions will always be 1 in this decoder, therefore:\r\n","        #output = [1, batch size, dec hid dim]\r\n","        #hidden = [1, batch size, dec hid dim]\r\n","        #this also means that output == hidden\r\n","        assert (output == hidden).all()\r\n","        \r\n","        embedded = embedded.squeeze(0)\r\n","        output = output.squeeze(0)\r\n","        weighted = weighted.squeeze(0)\r\n","        \r\n","        prediction = self.fc_out(torch.cat((output, weighted, embedded), dim = 1))\r\n","        \r\n","        #prediction = [batch size, output dim]\r\n","        \r\n","        return prediction, hidden.squeeze(0)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"LdR-KBRy8Xai"},"source":["class Seq2Seq(nn.Module):\r\n","    def __init__(self, encoder, decoder, device):\r\n","        super().__init__()\r\n","        \r\n","        self.encoder = encoder\r\n","        self.decoder = decoder\r\n","        self.device = device\r\n","        \r\n","    def forward(self, src, trg, teacher_forcing_ratio = 0.5):\r\n","        \r\n","        #src = [src len, batch size]\r\n","        #trg = [trg len, batch size]\r\n","        #teacher_forcing_ratio is probability to use teacher forcing\r\n","        #e.g. if teacher_forcing_ratio is 0.75 we use teacher forcing 75% of the time\r\n","        \r\n","        batch_size = src.shape[1]\r\n","        trg_len = trg.shape[0]\r\n","        trg_vocab_size = self.decoder.output_dim\r\n","        \r\n","        #tensor to store decoder outputs\r\n","        outputs = torch.zeros(trg_len, batch_size, trg_vocab_size).to(self.device)\r\n","        \r\n","        #encoder_outputs is all hidden states of the input sequence, back and forwards\r\n","        #hidden is the final forward and backward hidden states, passed through a linear layer\r\n","        encoder_outputs, hidden = self.encoder(src)\r\n","                \r\n","        #first input to the decoder is the <sos> tokens\r\n","        input = trg[0,:]\r\n","        \r\n","        for t in range(1, trg_len):\r\n","            \r\n","            #insert input token embedding, previous hidden state and all encoder hidden states\r\n","            #receive output tensor (predictions) and new hidden state\r\n","            output, hidden = self.decoder(input, hidden, encoder_outputs)\r\n","            \r\n","            #place predictions in a tensor holding predictions for each token\r\n","            outputs[t] = output\r\n","            \r\n","            #decide if we are going to use teacher forcing or not\r\n","            teacher_force = random.random() < teacher_forcing_ratio\r\n","            \r\n","            #get the highest predicted token from our predictions\r\n","            top1 = output.argmax(1) \r\n","            \r\n","            #if teacher forcing, use actual next token as next input\r\n","            #if not, use predicted token\r\n","            input = trg[t] if teacher_force else top1\r\n","\r\n","        return outputs"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"7peQ2amZ8XXp"},"source":["INPUT_DIM = len(SRC.vocab)\r\n","OUTPUT_DIM = len(TRG.vocab)\r\n","ENC_EMB_DIM = 256\r\n","DEC_EMB_DIM = 256\r\n","ENC_HID_DIM = 512\r\n","DEC_HID_DIM = 512\r\n","ENC_DROPOUT = 0.5\r\n","DEC_DROPOUT = 0.5\r\n","\r\n","attn = Attention(ENC_HID_DIM, DEC_HID_DIM)\r\n","enc = Encoder(INPUT_DIM, ENC_EMB_DIM, ENC_HID_DIM, DEC_HID_DIM, ENC_DROPOUT)\r\n","dec = Decoder(OUTPUT_DIM, DEC_EMB_DIM, ENC_HID_DIM, DEC_HID_DIM, DEC_DROPOUT, attn)\r\n","\r\n","model = Seq2Seq(enc, dec, device).to(device)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4YvZ0PqdBQBR","outputId":"8072e47b-1555-403e-81cb-9698dc39105c"},"source":["def init_weights(m):\r\n","    for name, param in m.named_parameters():\r\n","        if 'weight' in name:\r\n","            nn.init.normal_(param.data, mean=0, std=0.01)\r\n","        else:\r\n","            nn.init.constant_(param.data, 0)\r\n","            \r\n","model.apply(init_weights)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["Seq2Seq(\n","  (encoder): Encoder(\n","    (embedding): Embedding(3087, 256)\n","    (rnn): GRU(256, 512, bidirectional=True)\n","    (fc): Linear(in_features=1024, out_features=512, bias=True)\n","    (dropout): Dropout(p=0.5, inplace=False)\n","  )\n","  (decoder): Decoder(\n","    (attention): Attention(\n","      (attn): Linear(in_features=1536, out_features=512, bias=True)\n","      (v): Linear(in_features=512, out_features=1, bias=False)\n","    )\n","    (embedding): Embedding(4483, 256)\n","    (rnn): GRU(1280, 512)\n","    (fc_out): Linear(in_features=1792, out_features=4483, bias=True)\n","    (dropout): Dropout(p=0.5, inplace=False)\n","  )\n",")"]},"metadata":{"tags":[]},"execution_count":25}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dXvKyuOUBP-r","outputId":"9b87b94f-36b4-4273-f5a9-6209eeeddba5"},"source":["def count_parameters(model):\r\n","    return sum(p.numel() for p in model.parameters() if p.requires_grad)\r\n","\r\n","print(f'The model has {count_parameters(model):,} trainable parameters')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["The model has 16,409,219 trainable parameters\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"hIX2Vi_bBP7z"},"source":["optimizer = optim.Adam(model.parameters())"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"fcrMYlCPBP5C"},"source":["TRG_PAD_IDX = TRG.vocab.stoi[TRG.pad_token]\r\n","\r\n","criterion = nn.CrossEntropyLoss(ignore_index = TRG_PAD_IDX)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"AitifQcDBP2P"},"source":["def train(model, iterator, optimizer, criterion, clip):\r\n","    \r\n","    model.train()\r\n","    \r\n","    epoch_loss = 0\r\n","    \r\n","    for i, batch in enumerate(iterator):\r\n","        \r\n","        src = batch.src\r\n","        trg = batch.trg\r\n","        \r\n","        optimizer.zero_grad()\r\n","        \r\n","        output = model(src, trg)\r\n","        \r\n","        #trg = [trg len, batch size]\r\n","        #output = [trg len, batch size, output dim]\r\n","        \r\n","        output_dim = output.shape[-1]\r\n","        \r\n","        output = output[1:].view(-1, output_dim)\r\n","        trg = trg[1:].view(-1)\r\n","        \r\n","        #trg = [(trg len - 1) * batch size]\r\n","        #output = [(trg len - 1) * batch size, output dim]\r\n","        \r\n","        loss = criterion(output, trg)\r\n","        \r\n","        loss.backward()\r\n","        \r\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\r\n","        \r\n","        optimizer.step()\r\n","        \r\n","        epoch_loss += loss.item()\r\n","        \r\n","    return epoch_loss / len(iterator)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"7qIZ1hxABPzY"},"source":["def evaluate(model, iterator, criterion):\r\n","    \r\n","    model.eval()\r\n","    \r\n","    epoch_loss = 0\r\n","    \r\n","    with torch.no_grad():\r\n","    \r\n","        for i, batch in enumerate(iterator):\r\n","\r\n","            src = batch.src\r\n","            trg = batch.trg\r\n","\r\n","            output = model(src, trg, 0) #turn off teacher forcing\r\n","\r\n","            #trg = [trg len, batch size]\r\n","            #output = [trg len, batch size, output dim]\r\n","\r\n","            output_dim = output.shape[-1]\r\n","            \r\n","            output = output[1:].view(-1, output_dim)\r\n","            trg = trg[1:].view(-1)\r\n","\r\n","            #trg = [(trg len - 1) * batch size]\r\n","            #output = [(trg len - 1) * batch size, output dim]\r\n","\r\n","            loss = criterion(output, trg)\r\n","\r\n","            epoch_loss += loss.item()\r\n","        \r\n","    return epoch_loss / len(iterator)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"usH9lSZvBPwg"},"source":["def epoch_time(start_time, end_time):\r\n","    elapsed_time = end_time - start_time\r\n","    elapsed_mins = int(elapsed_time / 60)\r\n","    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\r\n","    return elapsed_mins, elapsed_secs"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"GhUHEcXnBPoK"},"source":["N_EPOCHS = 2\r\n","CLIP = 1\r\n","\r\n","best_valid_loss = float('inf')\r\n","\r\n","for epoch in range(N_EPOCHS):\r\n","    \r\n","    start_time = time.time()\r\n","    \r\n","    train_loss = train(model, train_iterator, optimizer, criterion, CLIP)\r\n","    valid_loss = evaluate(model, valid_iterator, criterion)\r\n","    \r\n","    end_time = time.time()\r\n","    \r\n","    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\r\n","    \r\n","    if valid_loss < best_valid_loss:\r\n","        best_valid_loss = valid_loss\r\n","        torch.save(model.state_dict(), 'tut3-model.pt')\r\n","    \r\n","    print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\r\n","    print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}')\r\n","    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. PPL: {math.exp(valid_loss):7.3f}')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"U40BDR8JjDY-"},"source":["## Performance: Test Loss=3.370"]},{"cell_type":"code","metadata":{"id":"9lu6ksmg8XVC"},"source":["model.load_state_dict(torch.load('tut3-model.pt'))\r\n","\r\n","test_loss = evaluate(model, test_iterator, criterion)\r\n","\r\n","print(f'| Test Loss: {test_loss:.3f} | Test PPL: {math.exp(test_loss):7.3f} |')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"uRfFaFgeizxV"},"source":["# Tutorial 4"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"cpG7ve_SZ39x","outputId":"a5c094be-365a-4c3e-b675-f54661316be0"},"source":["!pip install torch==1.6.0 torchvision==0.7.0"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: torch==1.6.0 in /usr/local/lib/python3.6/dist-packages (1.6.0)\n","Requirement already satisfied: torchvision==0.7.0 in /usr/local/lib/python3.6/dist-packages (0.7.0)\n","Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch==1.6.0) (0.16.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torch==1.6.0) (1.18.5)\n","Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.6/dist-packages (from torchvision==0.7.0) (7.0.0)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6fX2KFIzaCWL","outputId":"5e9da851-614e-40f0-fa63-4ecc469f93e3"},"source":["!pip install torchtext==0.6.0"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: torchtext==0.6.0 in /usr/local/lib/python3.6/dist-packages (0.6.0)\n","Requirement already satisfied: sentencepiece in /usr/local/lib/python3.6/dist-packages (from torchtext==0.6.0) (0.1.94)\n","Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from torchtext==0.6.0) (1.15.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torchtext==0.6.0) (1.18.5)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from torchtext==0.6.0) (4.41.1)\n","Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (from torchtext==0.6.0) (1.6.0)\n","Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from torchtext==0.6.0) (2.23.0)\n","Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch->torchtext==0.6.0) (0.16.0)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->torchtext==0.6.0) (3.0.4)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->torchtext==0.6.0) (2.10)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->torchtext==0.6.0) (1.24.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->torchtext==0.6.0) (2020.12.5)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"ikN5DldiVh4p"},"source":["def tokenize_zh(text):\r\n","    \"\"\"\r\n","    Tokenizes German text from a string into a list of strings\r\n","    \"\"\"\r\n","    return [tok.text for tok in spacy_zh.tokenizer(text)]\r\n","\r\n","def tokenize_en(text):\r\n","    \"\"\"\r\n","    Tokenizes English text from a string into a list of strings\r\n","    \"\"\"\r\n","    return [tok.text for tok in spacy_en.tokenizer(text)]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"PydDW1cOXWs_"},"source":["SRC = Field(tokenize = tokenize_en, \r\n","            init_token = '<sos>', \r\n","            eos_token = '<eos>', \r\n","            lower = True,\r\n","            include_lengths = True)\r\n","\r\n","TRG = Field(tokenize = tokenize_zh, \r\n","            init_token = '<sos>', \r\n","            eos_token = '<eos>', \r\n","            lower = True)#,batch_first = True"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Xv9eMYLJXWtC"},"source":["import torchtext\r\n","\r\n","dataset = torchtext.data.TabularDataset(\r\n","    path='/content/drive/MyDrive/NLP/pj2/project.csv',\r\n","    format='csv',\r\n","    skip_header=True,\r\n","    fields=[('src', SRC), ('trg', TRG)]\r\n",")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"VjkvfqZvXWtD"},"source":["train_data, valid_data, test_data = dataset.split(split_ratio=[0.6, 0.2, 0.2])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"UODXuJ3iXWtE","outputId":"e8616eb8-1c69-4329-b20a-2dac212b5ef4"},"source":["print(f\"Number of training examples: {len(train_data.examples)}\")\r\n","print(f\"Number of validation examples: {len(valid_data.examples)}\")\r\n","print(f\"Number of testing examples: {len(test_data.examples)}\")"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Number of training examples: 14416\n","Number of validation examples: 4805\n","Number of testing examples: 4805\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"b-uQTh2oXWtE","outputId":"6fa8efe7-29ff-49c8-9a6c-e8c823d79920"},"source":["print(vars(train_data.examples[0]))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["{'src': ['the', 'bicycle', 'by', 'the', 'door', 'is', 'mine', '.'], 'trg': ['在', '門口', '的', '自行車', '是', '我', '的', '。']}\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"SmtJ5WczXWtF"},"source":["SRC.build_vocab(train_data, min_freq = 2)\r\n","TRG.build_vocab(train_data, min_freq = 2)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bWuWmhYiXWtF","outputId":"c6787251-e6f5-4000-acd3-b2a7a4ef7afe"},"source":["print(f\"Unique tokens in source (zh) vocabulary: {len(SRC.vocab)}\")\r\n","print(f\"Unique tokens in target (en) vocabulary: {len(TRG.vocab)}\")"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Unique tokens in source (zh) vocabulary: 3087\n","Unique tokens in target (en) vocabulary: 4483\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"zuhbRrSkXWtF"},"source":["device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"PwtbxJVkXWtF"},"source":["BATCH_SIZE = 128\r\n","\r\n","train_iterator, valid_iterator, test_iterator = BucketIterator.splits(\r\n","    (train_data, valid_data, test_data),\r\n","     sort_within_batch = True, ##\r\n","     batch_size = BATCH_SIZE,\r\n","     sort_key = lambda x : len(x.src),\r\n","     device = device)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"PgRnqTPvXWtF"},"source":["class Encoder(nn.Module):\r\n","    def __init__(self, input_dim, emb_dim, enc_hid_dim, dec_hid_dim, dropout):\r\n","        super().__init__()\r\n","        \r\n","        self.embedding = nn.Embedding(input_dim, emb_dim)\r\n","        \r\n","        self.rnn = nn.GRU(emb_dim, enc_hid_dim, bidirectional = True)\r\n","        \r\n","        self.fc = nn.Linear(enc_hid_dim * 2, dec_hid_dim)\r\n","        \r\n","        self.dropout = nn.Dropout(dropout)\r\n","        \r\n","    def forward(self, src, src_len):\r\n","        \r\n","        #src = [src len, batch size]\r\n","        #src_len = [batch size]\r\n","        \r\n","        embedded = self.dropout(self.embedding(src))\r\n","        \r\n","        #embedded = [src len, batch size, emb dim]\r\n","                \r\n","        packed_embedded = nn.utils.rnn.pack_padded_sequence(embedded, src_len)\r\n","                \r\n","        packed_outputs, hidden = self.rnn(packed_embedded)\r\n","                                 \r\n","        #packed_outputs is a packed sequence containing all hidden states\r\n","        #hidden is now from the final non-padded element in the batch\r\n","            \r\n","        outputs, _ = nn.utils.rnn.pad_packed_sequence(packed_outputs) \r\n","            \r\n","        #outputs is now a non-packed sequence, all hidden states obtained\r\n","        #  when the input is a pad token are all zeros\r\n","            \r\n","        #outputs = [src len, batch size, hid dim * num directions]\r\n","        #hidden = [n layers * num directions, batch size, hid dim]\r\n","        \r\n","        #hidden is stacked [forward_1, backward_1, forward_2, backward_2, ...]\r\n","        #outputs are always from the last layer\r\n","        \r\n","        #hidden [-2, :, : ] is the last of the forwards RNN \r\n","        #hidden [-1, :, : ] is the last of the backwards RNN\r\n","        \r\n","        #initial decoder hidden is final hidden state of the forwards and backwards \r\n","        #  encoder RNNs fed through a linear layer\r\n","        hidden = torch.tanh(self.fc(torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim = 1)))\r\n","        \r\n","        #outputs = [src len, batch size, enc hid dim * 2]\r\n","        #hidden = [batch size, dec hid dim]\r\n","        \r\n","        return outputs, hidden"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Irbgh15JYYox"},"source":["class Attention(nn.Module):\r\n","    def __init__(self, enc_hid_dim, dec_hid_dim):\r\n","        super().__init__()\r\n","        \r\n","        self.attn = nn.Linear((enc_hid_dim * 2) + dec_hid_dim, dec_hid_dim)\r\n","        self.v = nn.Linear(dec_hid_dim, 1, bias = False)\r\n","        \r\n","    def forward(self, hidden, encoder_outputs, mask):\r\n","        \r\n","        #hidden = [batch size, dec hid dim]\r\n","        #encoder_outputs = [src len, batch size, enc hid dim * 2]\r\n","        \r\n","        batch_size = encoder_outputs.shape[1]\r\n","        src_len = encoder_outputs.shape[0]\r\n","        \r\n","        #repeat decoder hidden state src_len times\r\n","        hidden = hidden.unsqueeze(1).repeat(1, src_len, 1)\r\n","  \r\n","        encoder_outputs = encoder_outputs.permute(1, 0, 2)\r\n","        \r\n","        #hidden = [batch size, src len, dec hid dim]\r\n","        #encoder_outputs = [batch size, src len, enc hid dim * 2]\r\n","        \r\n","        energy = torch.tanh(self.attn(torch.cat((hidden, encoder_outputs), dim = 2))) \r\n","        \r\n","        #energy = [batch size, src len, dec hid dim]\r\n","\r\n","        attention = self.v(energy).squeeze(2)\r\n","        \r\n","        #attention = [batch size, src len]\r\n","        \r\n","        attention = attention.masked_fill(mask == 0, -1e10)\r\n","        \r\n","        return F.softmax(attention, dim = 1)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"QOn1WzKZXWtG"},"source":["class Decoder(nn.Module):\r\n","    def __init__(self, output_dim, emb_dim, enc_hid_dim, dec_hid_dim, dropout, attention):\r\n","        super().__init__()\r\n","\r\n","        self.output_dim = output_dim\r\n","        self.attention = attention\r\n","        \r\n","        self.embedding = nn.Embedding(output_dim, emb_dim)\r\n","        \r\n","        self.rnn = nn.GRU((enc_hid_dim * 2) + emb_dim, dec_hid_dim)\r\n","        \r\n","        self.fc_out = nn.Linear((enc_hid_dim * 2) + dec_hid_dim + emb_dim, output_dim)\r\n","        \r\n","        self.dropout = nn.Dropout(dropout)\r\n","        \r\n","    def forward(self, input, hidden, encoder_outputs, mask):\r\n","             \r\n","        #input = [batch size]\r\n","        #hidden = [batch size, dec hid dim]\r\n","        #encoder_outputs = [src len, batch size, enc hid dim * 2]\r\n","        #mask = [batch size, src len]\r\n","        \r\n","        input = input.unsqueeze(0)\r\n","        \r\n","        #input = [1, batch size]\r\n","        \r\n","        embedded = self.dropout(self.embedding(input))\r\n","        \r\n","        #embedded = [1, batch size, emb dim]\r\n","        \r\n","        a = self.attention(hidden, encoder_outputs, mask)\r\n","                \r\n","        #a = [batch size, src len]\r\n","        \r\n","        a = a.unsqueeze(1)\r\n","        \r\n","        #a = [batch size, 1, src len]\r\n","        \r\n","        encoder_outputs = encoder_outputs.permute(1, 0, 2)\r\n","        \r\n","        #encoder_outputs = [batch size, src len, enc hid dim * 2]\r\n","        \r\n","        weighted = torch.bmm(a, encoder_outputs)\r\n","        \r\n","        #weighted = [batch size, 1, enc hid dim * 2]\r\n","        \r\n","        weighted = weighted.permute(1, 0, 2)\r\n","        \r\n","        #weighted = [1, batch size, enc hid dim * 2]\r\n","        \r\n","        rnn_input = torch.cat((embedded, weighted), dim = 2)\r\n","        \r\n","        #rnn_input = [1, batch size, (enc hid dim * 2) + emb dim]\r\n","            \r\n","        output, hidden = self.rnn(rnn_input, hidden.unsqueeze(0))\r\n","        \r\n","        #output = [seq len, batch size, dec hid dim * n directions]\r\n","        #hidden = [n layers * n directions, batch size, dec hid dim]\r\n","        \r\n","        #seq len, n layers and n directions will always be 1 in this decoder, therefore:\r\n","        #output = [1, batch size, dec hid dim]\r\n","        #hidden = [1, batch size, dec hid dim]\r\n","        #this also means that output == hidden\r\n","        assert (output == hidden).all()\r\n","        \r\n","        embedded = embedded.squeeze(0)\r\n","        output = output.squeeze(0)\r\n","        weighted = weighted.squeeze(0)\r\n","        \r\n","        prediction = self.fc_out(torch.cat((output, weighted, embedded), dim = 1))\r\n","        \r\n","        #prediction = [batch size, output dim]\r\n","        \r\n","        return prediction, hidden.squeeze(0), a.squeeze(1)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"gYYo8j6YXWtG"},"source":["class Seq2Seq(nn.Module):\r\n","    def __init__(self, encoder, decoder, src_pad_idx, device):\r\n","        super().__init__()\r\n","        \r\n","        self.encoder = encoder\r\n","        self.decoder = decoder\r\n","        self.src_pad_idx = src_pad_idx\r\n","        self.device = device\r\n","        \r\n","    def create_mask(self, src):\r\n","        mask = (src != self.src_pad_idx).permute(1, 0)\r\n","        return mask\r\n","        \r\n","    def forward(self, src, src_len, trg, teacher_forcing_ratio = 0.5):\r\n","        \r\n","        #src = [src len, batch size]\r\n","        #src_len = [batch size]\r\n","        #trg = [trg len, batch size]\r\n","        #teacher_forcing_ratio is probability to use teacher forcing\r\n","        #e.g. if teacher_forcing_ratio is 0.75 we use teacher forcing 75% of the time\r\n","                    \r\n","        batch_size = src.shape[1]\r\n","        trg_len = trg.shape[0]\r\n","        trg_vocab_size = self.decoder.output_dim\r\n","        \r\n","        #tensor to store decoder outputs\r\n","        outputs = torch.zeros(trg_len, batch_size, trg_vocab_size).to(self.device)\r\n","        \r\n","        #encoder_outputs is all hidden states of the input sequence, back and forwards\r\n","        #hidden is the final forward and backward hidden states, passed through a linear layer\r\n","        encoder_outputs, hidden = self.encoder(src, src_len)\r\n","                \r\n","        #first input to the decoder is the <sos> tokens\r\n","        input = trg[0,:]\r\n","        \r\n","        mask = self.create_mask(src)\r\n","\r\n","        #mask = [batch size, src len]\r\n","                \r\n","        for t in range(1, trg_len):\r\n","            \r\n","            #insert input token embedding, previous hidden state, all encoder hidden states \r\n","            #  and mask\r\n","            #receive output tensor (predictions) and new hidden state\r\n","            output, hidden, _ = self.decoder(input, hidden, encoder_outputs, mask)\r\n","            \r\n","            #place predictions in a tensor holding predictions for each token\r\n","            outputs[t] = output\r\n","            \r\n","            #decide if we are going to use teacher forcing or not\r\n","            teacher_force = random.random() < teacher_forcing_ratio\r\n","            \r\n","            #get the highest predicted token from our predictions\r\n","            top1 = output.argmax(1) \r\n","            \r\n","            #if teacher forcing, use actual next token as next input\r\n","            #if not, use predicted token\r\n","            input = trg[t] if teacher_force else top1\r\n","            \r\n","        return outputs"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"G2RCLy4GXWtG"},"source":["INPUT_DIM = len(SRC.vocab)\r\n","OUTPUT_DIM = len(TRG.vocab)\r\n","ENC_EMB_DIM = 256\r\n","DEC_EMB_DIM = 256\r\n","ENC_HID_DIM = 512\r\n","DEC_HID_DIM = 512\r\n","ENC_DROPOUT = 0.5\r\n","DEC_DROPOUT = 0.5\r\n","SRC_PAD_IDX = SRC.vocab.stoi[SRC.pad_token]\r\n","\r\n","attn = Attention(ENC_HID_DIM, DEC_HID_DIM)\r\n","enc = Encoder(INPUT_DIM, ENC_EMB_DIM, ENC_HID_DIM, DEC_HID_DIM, ENC_DROPOUT)\r\n","dec = Decoder(OUTPUT_DIM, DEC_EMB_DIM, ENC_HID_DIM, DEC_HID_DIM, DEC_DROPOUT, attn)\r\n","\r\n","model = Seq2Seq(enc, dec, SRC_PAD_IDX, device).to(device)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VlyKd4BNYtBL","outputId":"1f373a92-0c46-4257-b200-43562ab77775"},"source":["def init_weights(m):\r\n","    for name, param in m.named_parameters():\r\n","        if 'weight' in name:\r\n","            nn.init.normal_(param.data, mean=0, std=0.01)\r\n","        else:\r\n","            nn.init.constant_(param.data, 0)\r\n","            \r\n","model.apply(init_weights)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["Seq2Seq(\n","  (encoder): Encoder(\n","    (embedding): Embedding(3087, 256)\n","    (rnn): GRU(256, 512, bidirectional=True)\n","    (fc): Linear(in_features=1024, out_features=512, bias=True)\n","    (dropout): Dropout(p=0.5, inplace=False)\n","  )\n","  (decoder): Decoder(\n","    (attention): Attention(\n","      (attn): Linear(in_features=1536, out_features=512, bias=True)\n","      (v): Linear(in_features=512, out_features=1, bias=False)\n","    )\n","    (embedding): Embedding(4483, 256)\n","    (rnn): GRU(1280, 512)\n","    (fc_out): Linear(in_features=1792, out_features=4483, bias=True)\n","    (dropout): Dropout(p=0.5, inplace=False)\n","  )\n",")"]},"metadata":{"tags":[]},"execution_count":50}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"93haRHY6XWtG","outputId":"268281aa-aeca-4c19-843e-f8265183d1be"},"source":["def count_parameters(model):\r\n","    return sum(p.numel() for p in model.parameters() if p.requires_grad)\r\n","\r\n","print(f'The model has {count_parameters(model):,} trainable parameters')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["The model has 16,409,219 trainable parameters\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"0eopeq4iXWtG"},"source":["optimizer = optim.Adam(model.parameters())"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"aHUrZ3niXWtH"},"source":["TRG_PAD_IDX = TRG.vocab.stoi[TRG.pad_token]\r\n","\r\n","criterion = nn.CrossEntropyLoss(ignore_index = TRG_PAD_IDX)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"F7wcOrKkXWtH"},"source":["def train(model, iterator, optimizer, criterion, clip):\r\n","    \r\n","    model.train()\r\n","    \r\n","    epoch_loss = 0\r\n","    \r\n","    for i, batch in enumerate(iterator):\r\n","        \r\n","        src, src_len = batch.src\r\n","        trg = batch.trg\r\n","        \r\n","        optimizer.zero_grad()\r\n","        \r\n","        output = model(src, src_len, trg)\r\n","        \r\n","        #trg = [trg len, batch size]\r\n","        #output = [trg len, batch size, output dim]\r\n","        \r\n","        output_dim = output.shape[-1]\r\n","        \r\n","        output = output[1:].view(-1, output_dim)\r\n","        trg = trg[1:].view(-1)\r\n","        \r\n","        #trg = [(trg len - 1) * batch size]\r\n","        #output = [(trg len - 1) * batch size, output dim]\r\n","        \r\n","        loss = criterion(output, trg)\r\n","        \r\n","        loss.backward()\r\n","        \r\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\r\n","        \r\n","        optimizer.step()\r\n","        \r\n","        epoch_loss += loss.item()\r\n","        \r\n","    return epoch_loss / len(iterator)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"357X8S2SXWtH"},"source":["def evaluate(model, iterator, criterion):\r\n","    \r\n","    model.eval()\r\n","    \r\n","    epoch_loss = 0\r\n","    \r\n","    with torch.no_grad():\r\n","    \r\n","        for i, batch in enumerate(iterator):\r\n","\r\n","            src, src_len = batch.src\r\n","            trg = batch.trg\r\n","\r\n","            output = model(src, src_len, trg, 0) #turn off teacher forcing\r\n","            \r\n","            #trg = [trg len, batch size]\r\n","            #output = [trg len, batch size, output dim]\r\n","\r\n","            output_dim = output.shape[-1]\r\n","            \r\n","            output = output[1:].view(-1, output_dim)\r\n","            trg = trg[1:].view(-1)\r\n","\r\n","            #trg = [(trg len - 1) * batch size]\r\n","            #output = [(trg len - 1) * batch size, output dim]\r\n","\r\n","            loss = criterion(output, trg)\r\n","\r\n","            epoch_loss += loss.item()\r\n","        \r\n","    return epoch_loss / len(iterator)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"XlTFO-5zXWtH"},"source":["def epoch_time(start_time, end_time):\r\n","    elapsed_time = end_time - start_time\r\n","    elapsed_mins = int(elapsed_time / 60)\r\n","    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\r\n","    return elapsed_mins, elapsed_secs"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ljgWQ1VYXWtH","outputId":"1804300e-0894-4c8f-94ce-a9228661af26"},"source":["N_EPOCHS = 2\r\n","CLIP = 1\r\n","\r\n","best_valid_loss = float('inf')\r\n","\r\n","for epoch in range(N_EPOCHS):\r\n","    \r\n","    start_time = time.time()\r\n","    \r\n","    train_loss = train(model, train_iterator, optimizer, criterion, CLIP)\r\n","    valid_loss = evaluate(model, valid_iterator, criterion)\r\n","    \r\n","    end_time = time.time()\r\n","    \r\n","    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\r\n","    \r\n","    if valid_loss < best_valid_loss:\r\n","        best_valid_loss = valid_loss\r\n","        torch.save(model.state_dict(), 'tut4-model.pt')\r\n","    \r\n","    print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\r\n","    print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}')\r\n","    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. PPL: {math.exp(valid_loss):7.3f}')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Epoch: 01 | Time: 4m 56s\n","\tTrain Loss: 8.408 | Train PPL: 4482.797\n","\t Val. Loss: 8.408 |  Val. PPL: 4482.691\n","Epoch: 02 | Time: 4m 49s\n","\tTrain Loss: 8.408 | Train PPL: 4482.834\n","\t Val. Loss: 8.408 |  Val. PPL: 4482.691\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"xQCWXjXgjEkg"},"source":["## Performance: Test Loss=8.408"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"W3x7SbFUg4an","outputId":"71fe7499-1235-4a4b-dd6a-9057701001bf"},"source":["model.load_state_dict(torch.load('tut4-model.pt'))\r\n","\r\n","test_loss = evaluate(model, test_iterator, criterion)\r\n","\r\n","print(f'| Test Loss: {test_loss:.3f} | Test PPL: {math.exp(test_loss):7.3f} |')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["| Test Loss: 8.408 | Test PPL: 4482.722 |\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"BYe0tIroo8bo"},"source":["#Tutorial 5"]},{"cell_type":"code","metadata":{"id":"gCcaVrVAXu9m"},"source":["def tokenize_zh(text):\r\n","    \"\"\"\r\n","    Tokenizes Chinese text from a string into a list of strings (tokens) and reverses it\r\n","    \"\"\"\r\n","    return [tok.text for tok in spacy_zh.tokenizer(text)]#[::-1]\r\n","\r\n","def tokenize_en(text):\r\n","    \"\"\"\r\n","    Tokenizes English text from a string into a list of strings (tokens)\r\n","    \"\"\"\r\n","    return [tok.text for tok in spacy_en.tokenizer(text)]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"HuipSoo2X0Nl"},"source":["SRC = Field(tokenize = tokenize_en, \r\n","            init_token = '<sos>', \r\n","            eos_token = '<eos>', \r\n","            lower = True,\r\n","            batch_first = True)\r\n","\r\n","TRG = Field(tokenize = tokenize_zh, \r\n","            init_token = '<sos>', \r\n","            eos_token = '<eos>', \r\n","            lower = True,\r\n","            batch_first = True)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"SgXzsWqvbHTL"},"source":["import torchtext\r\n","\r\n","dataset = torchtext.data.TabularDataset(\r\n","    path='/content/drive/MyDrive/NLP/pj2/project.csv',\r\n","    format='csv',\r\n","    skip_header=True,\r\n","    fields=[('src', SRC), ('trg', TRG)]\r\n",")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ljFg_-uubLEK"},"source":["train_data, valid_data, test_data = dataset.split(split_ratio=[0.6, 0.2, 0.2])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3xWhx7zMYARX","outputId":"2fdf6e5a-5709-42ee-86f6-0a6a99e01877"},"source":["print(f\"Number of training examples: {len(train_data.examples)}\")\r\n","print(f\"Number of validation examples: {len(valid_data.examples)}\")\r\n","print(f\"Number of testing examples: {len(test_data.examples)}\")"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Number of training examples: 14416\n","Number of validation examples: 4805\n","Number of testing examples: 4805\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bhsauRfKYtBU","outputId":"fd55fd93-20f8-4e24-8924-29eb42a1d971"},"source":["print(vars(train_data.examples[0]))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["{'src': ['the', 'bicycle', 'by', 'the', 'door', 'is', 'mine', '.'], 'trg': ['在', '門口', '的', '自行車', '是', '我', '的', '。']}\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"sr5T0IeFYtDV"},"source":["SRC.build_vocab(train_data, min_freq = 2)\r\n","TRG.build_vocab(train_data, min_freq = 2)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bwwV9gGCYtFf","outputId":"3d4bd983-3af9-4773-a03b-abd2e4ca9e9a"},"source":["print(f\"Unique tokens in source (zh) vocabulary: {len(SRC.vocab)}\")\r\n","print(f\"Unique tokens in target (en) vocabulary: {len(TRG.vocab)}\")"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Unique tokens in source (zh) vocabulary: 3087\n","Unique tokens in target (en) vocabulary: 4483\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"OR4dga-SmuKj"},"source":["device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"tfa4z3PhpkNA"},"source":["BATCH_SIZE = 128\r\n","\r\n","train_iterator, valid_iterator, test_iterator = BucketIterator.splits(\r\n","    (train_data, valid_data, test_data),\r\n","     sort=False, \r\n","     batch_size = BATCH_SIZE,\r\n","     device = device)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"vRcDpgc9pouB"},"source":["class Encoder(nn.Module):\r\n","    def __init__(self, \r\n","                 input_dim, \r\n","                 emb_dim, \r\n","                 hid_dim, \r\n","                 n_layers, \r\n","                 kernel_size, \r\n","                 dropout, \r\n","                 device,\r\n","                 max_length = 100):\r\n","        super().__init__()\r\n","        \r\n","        assert kernel_size % 2 == 1, \"Kernel size must be odd!\"\r\n","        \r\n","        self.device = device\r\n","        \r\n","        self.scale = torch.sqrt(torch.FloatTensor([0.5])).to(device)\r\n","        \r\n","        self.tok_embedding = nn.Embedding(input_dim, emb_dim)\r\n","        self.pos_embedding = nn.Embedding(max_length, emb_dim)\r\n","        \r\n","        self.emb2hid = nn.Linear(emb_dim, hid_dim)\r\n","        self.hid2emb = nn.Linear(hid_dim, emb_dim)\r\n","        \r\n","        self.convs = nn.ModuleList([nn.Conv1d(in_channels = hid_dim, \r\n","                                              out_channels = 2 * hid_dim, \r\n","                                              kernel_size = kernel_size, \r\n","                                              padding = (kernel_size - 1) // 2)\r\n","                                    for _ in range(n_layers)])\r\n","        \r\n","        self.dropout = nn.Dropout(dropout)\r\n","        \r\n","    def forward(self, src):\r\n","        \r\n","        #src = [batch size, src len]\r\n","        \r\n","        batch_size = src.shape[0]\r\n","        src_len = src.shape[1]\r\n","        \r\n","        #create position tensor\r\n","        pos = torch.arange(0, src_len).unsqueeze(0).repeat(batch_size, 1).to(self.device)\r\n","        \r\n","        #pos = [0, 1, 2, 3, ..., src len - 1]\r\n","        \r\n","        #pos = [batch size, src len]\r\n","        \r\n","        #embed tokens and positions\r\n","        tok_embedded = self.tok_embedding(src)\r\n","        pos_embedded = self.pos_embedding(pos)\r\n","        \r\n","        #tok_embedded = pos_embedded = [batch size, src len, emb dim]\r\n","        \r\n","        #combine embeddings by elementwise summing\r\n","        embedded = self.dropout(tok_embedded + pos_embedded)\r\n","        \r\n","        #embedded = [batch size, src len, emb dim]\r\n","        \r\n","        #pass embedded through linear layer to convert from emb dim to hid dim\r\n","        conv_input = self.emb2hid(embedded)\r\n","        \r\n","        #conv_input = [batch size, src len, hid dim]\r\n","        \r\n","        #permute for convolutional layer\r\n","        conv_input = conv_input.permute(0, 2, 1) \r\n","        \r\n","        #conv_input = [batch size, hid dim, src len]\r\n","        \r\n","        #begin convolutional blocks...\r\n","        \r\n","        for i, conv in enumerate(self.convs):\r\n","        \r\n","            #pass through convolutional layer\r\n","            conved = conv(self.dropout(conv_input))\r\n","\r\n","            #conved = [batch size, 2 * hid dim, src len]\r\n","\r\n","            #pass through GLU activation function\r\n","            conved = F.glu(conved, dim = 1)\r\n","\r\n","            #conved = [batch size, hid dim, src len]\r\n","            \r\n","            #apply residual connection\r\n","            conved = (conved + conv_input) * self.scale\r\n","\r\n","            #conved = [batch size, hid dim, src len]\r\n","            \r\n","            #set conv_input to conved for next loop iteration\r\n","            conv_input = conved\r\n","        \r\n","        #...end convolutional blocks\r\n","        \r\n","        #permute and convert back to emb dim\r\n","        conved = self.hid2emb(conved.permute(0, 2, 1))\r\n","        \r\n","        #conved = [batch size, src len, emb dim]\r\n","        \r\n","        #elementwise sum output (conved) and input (embedded) to be used for attention\r\n","        combined = (conved + embedded) * self.scale\r\n","        \r\n","        #combined = [batch size, src len, emb dim]\r\n","        \r\n","        return conved, combined"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"WFC7BEKWj_UH"},"source":["class Decoder(nn.Module):\r\n","    def __init__(self, \r\n","                 output_dim, \r\n","                 emb_dim, \r\n","                 hid_dim, \r\n","                 n_layers, \r\n","                 kernel_size, \r\n","                 dropout, \r\n","                 trg_pad_idx, \r\n","                 device,\r\n","                 max_length = 100):\r\n","        super().__init__()\r\n","        \r\n","        self.kernel_size = kernel_size\r\n","        self.trg_pad_idx = trg_pad_idx\r\n","        self.device = device\r\n","        \r\n","        self.scale = torch.sqrt(torch.FloatTensor([0.5])).to(device)\r\n","        \r\n","        self.tok_embedding = nn.Embedding(output_dim, emb_dim)\r\n","        self.pos_embedding = nn.Embedding(max_length, emb_dim)\r\n","        \r\n","        self.emb2hid = nn.Linear(emb_dim, hid_dim)\r\n","        self.hid2emb = nn.Linear(hid_dim, emb_dim)\r\n","        \r\n","        self.attn_hid2emb = nn.Linear(hid_dim, emb_dim)\r\n","        self.attn_emb2hid = nn.Linear(emb_dim, hid_dim)\r\n","        \r\n","        self.fc_out = nn.Linear(emb_dim, output_dim)\r\n","        \r\n","        self.convs = nn.ModuleList([nn.Conv1d(in_channels = hid_dim, \r\n","                                              out_channels = 2 * hid_dim, \r\n","                                              kernel_size = kernel_size)\r\n","                                    for _ in range(n_layers)])\r\n","        \r\n","        self.dropout = nn.Dropout(dropout)\r\n","      \r\n","    def calculate_attention(self, embedded, conved, encoder_conved, encoder_combined):\r\n","        \r\n","        #embedded = [batch size, trg len, emb dim]\r\n","        #conved = [batch size, hid dim, trg len]\r\n","        #encoder_conved = encoder_combined = [batch size, src len, emb dim]\r\n","        \r\n","        #permute and convert back to emb dim\r\n","        conved_emb = self.attn_hid2emb(conved.permute(0, 2, 1))\r\n","        \r\n","        #conved_emb = [batch size, trg len, emb dim]\r\n","        \r\n","        combined = (conved_emb + embedded) * self.scale\r\n","        \r\n","        #combined = [batch size, trg len, emb dim]\r\n","                \r\n","        energy = torch.matmul(combined, encoder_conved.permute(0, 2, 1))\r\n","        \r\n","        #energy = [batch size, trg len, src len]\r\n","        \r\n","        attention = F.softmax(energy, dim=2)\r\n","        \r\n","        #attention = [batch size, trg len, src len]\r\n","            \r\n","        attended_encoding = torch.matmul(attention, encoder_combined)\r\n","        \r\n","        #attended_encoding = [batch size, trg len, emd dim]\r\n","        \r\n","        #convert from emb dim -> hid dim\r\n","        attended_encoding = self.attn_emb2hid(attended_encoding)\r\n","        \r\n","        #attended_encoding = [batch size, trg len, hid dim]\r\n","        \r\n","        #apply residual connection\r\n","        attended_combined = (conved + attended_encoding.permute(0, 2, 1)) * self.scale\r\n","        \r\n","        #attended_combined = [batch size, hid dim, trg len]\r\n","        \r\n","        return attention, attended_combined\r\n","        \r\n","    def forward(self, trg, encoder_conved, encoder_combined):\r\n","        \r\n","        #trg = [batch size, trg len]\r\n","        #encoder_conved = encoder_combined = [batch size, src len, emb dim]\r\n","                \r\n","        batch_size = trg.shape[0]\r\n","        trg_len = trg.shape[1]\r\n","            \r\n","        #create position tensor\r\n","        pos = torch.arange(0, trg_len).unsqueeze(0).repeat(batch_size, 1).to(self.device)\r\n","        \r\n","        #pos = [batch size, trg len]\r\n","        \r\n","        #embed tokens and positions\r\n","        tok_embedded = self.tok_embedding(trg)\r\n","        pos_embedded = self.pos_embedding(pos)\r\n","        \r\n","        #tok_embedded = [batch size, trg len, emb dim]\r\n","        #pos_embedded = [batch size, trg len, emb dim]\r\n","        \r\n","        #combine embeddings by elementwise summing\r\n","        embedded = self.dropout(tok_embedded + pos_embedded)\r\n","        \r\n","        #embedded = [batch size, trg len, emb dim]\r\n","        \r\n","        #pass embedded through linear layer to go through emb dim -> hid dim\r\n","        conv_input = self.emb2hid(embedded)\r\n","        \r\n","        #conv_input = [batch size, trg len, hid dim]\r\n","        \r\n","        #permute for convolutional layer\r\n","        conv_input = conv_input.permute(0, 2, 1) \r\n","        \r\n","        #conv_input = [batch size, hid dim, trg len]\r\n","        \r\n","        batch_size = conv_input.shape[0]\r\n","        hid_dim = conv_input.shape[1]\r\n","        \r\n","        for i, conv in enumerate(self.convs):\r\n","        \r\n","            #apply dropout\r\n","            conv_input = self.dropout(conv_input)\r\n","        \r\n","            #need to pad so decoder can't \"cheat\"\r\n","            padding = torch.zeros(batch_size, \r\n","                                  hid_dim, \r\n","                                  self.kernel_size - 1).fill_(self.trg_pad_idx).to(self.device)\r\n","                \r\n","            padded_conv_input = torch.cat((padding, conv_input), dim = 2)\r\n","        \r\n","            #padded_conv_input = [batch size, hid dim, trg len + kernel size - 1]\r\n","        \r\n","            #pass through convolutional layer\r\n","            conved = conv(padded_conv_input)\r\n","\r\n","            #conved = [batch size, 2 * hid dim, trg len]\r\n","            \r\n","            #pass through GLU activation function\r\n","            conved = F.glu(conved, dim = 1)\r\n","\r\n","            #conved = [batch size, hid dim, trg len]\r\n","            \r\n","            #calculate attention\r\n","            attention, conved = self.calculate_attention(embedded, \r\n","                                                         conved, \r\n","                                                         encoder_conved, \r\n","                                                         encoder_combined)\r\n","            \r\n","            #attention = [batch size, trg len, src len]\r\n","            \r\n","            #apply residual connection\r\n","            conved = (conved + conv_input) * self.scale\r\n","            \r\n","            #conved = [batch size, hid dim, trg len]\r\n","            \r\n","            #set conv_input to conved for next loop iteration\r\n","            conv_input = conved\r\n","            \r\n","        conved = self.hid2emb(conved.permute(0, 2, 1))\r\n","         \r\n","        #conved = [batch size, trg len, emb dim]\r\n","            \r\n","        output = self.fc_out(self.dropout(conved))\r\n","        \r\n","        #output = [batch size, trg len, output dim]\r\n","            \r\n","        return output, attention"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"HZ-_oDG2kRbs"},"source":["class Seq2Seq(nn.Module):\r\n","    def __init__(self, encoder, decoder):\r\n","        super().__init__()\r\n","        \r\n","        self.encoder = encoder\r\n","        self.decoder = decoder\r\n","        \r\n","    def forward(self, src, trg):\r\n","        \r\n","        #src = [batch size, src len]\r\n","        #trg = [batch size, trg len - 1] (<eos> token sliced off the end)\r\n","           \r\n","        #calculate z^u (encoder_conved) and (z^u + e) (encoder_combined)\r\n","        #encoder_conved is output from final encoder conv. block\r\n","        #encoder_combined is encoder_conved plus (elementwise) src embedding plus \r\n","        #  positional embeddings \r\n","        encoder_conved, encoder_combined = self.encoder(src)\r\n","            \r\n","        #encoder_conved = [batch size, src len, emb dim]\r\n","        #encoder_combined = [batch size, src len, emb dim]\r\n","        \r\n","        #calculate predictions of next words\r\n","        #output is a batch of predictions for each word in the trg sentence\r\n","        #attention a batch of attention scores across the src sentence for \r\n","        #  each word in the trg sentence\r\n","        output, attention = self.decoder(trg, encoder_conved, encoder_combined)\r\n","        \r\n","        #output = [batch size, trg len - 1, output dim]\r\n","        #attention = [batch size, trg len - 1, src len]\r\n","        \r\n","        return output, attention"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"5qcEMOyzkWM3"},"source":["INPUT_DIM = len(SRC.vocab)\r\n","OUTPUT_DIM = len(TRG.vocab)\r\n","EMB_DIM = 256\r\n","HID_DIM = 512 # each conv. layer has 2 * hid_dim filters\r\n","ENC_LAYERS = 10 # number of conv. blocks in encoder\r\n","DEC_LAYERS = 10 # number of conv. blocks in decoder\r\n","ENC_KERNEL_SIZE = 3 # must be odd!\r\n","DEC_KERNEL_SIZE = 3 # can be even or odd\r\n","ENC_DROPOUT = 0.25\r\n","DEC_DROPOUT = 0.25\r\n","TRG_PAD_IDX = TRG.vocab.stoi[TRG.pad_token]\r\n","    \r\n","enc = Encoder(INPUT_DIM, EMB_DIM, HID_DIM, ENC_LAYERS, ENC_KERNEL_SIZE, ENC_DROPOUT, device)\r\n","dec = Decoder(OUTPUT_DIM, EMB_DIM, HID_DIM, DEC_LAYERS, DEC_KERNEL_SIZE, DEC_DROPOUT, TRG_PAD_IDX, device)\r\n","\r\n","model = Seq2Seq(enc, dec).to(device)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pWY-fkj1kb-s","outputId":"f7b17ce6-2230-42f6-83f8-c50afb492c0a"},"source":["def count_parameters(model):\r\n","    return sum(p.numel() for p in model.parameters() if p.requires_grad)\r\n","\r\n","print(f'The model has {count_parameters(model):,} trainable parameters')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["The model has 35,407,747 trainable parameters\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"cM2qyB8dke1z"},"source":["optimizer = optim.Adam(model.parameters())"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"EAVfi3GKkhnO"},"source":["criterion = nn.CrossEntropyLoss(ignore_index = TRG_PAD_IDX)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Q4sFtyxNkkdd"},"source":["def train(model, iterator, optimizer, criterion, clip):\r\n","    \r\n","    model.train()\r\n","    \r\n","    epoch_loss = 0\r\n","    \r\n","    for i, batch in enumerate(iterator):\r\n","        \r\n","        src = batch.src\r\n","        trg = batch.trg\r\n","        \r\n","        optimizer.zero_grad()\r\n","        \r\n","        output, _ = model(src, trg[:,:-1])\r\n","        \r\n","        #output = [batch size, trg len - 1, output dim]\r\n","        #trg = [batch size, trg len]\r\n","        \r\n","        output_dim = output.shape[-1]\r\n","        \r\n","        output = output.contiguous().view(-1, output_dim)\r\n","        trg = trg[:,1:].contiguous().view(-1)\r\n","        \r\n","        #output = [batch size * trg len - 1, output dim]\r\n","        #trg = [batch size * trg len - 1]\r\n","        \r\n","        loss = criterion(output, trg)\r\n","        \r\n","        loss.backward()\r\n","        \r\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\r\n","        \r\n","        optimizer.step()\r\n","        \r\n","        epoch_loss += loss.item()\r\n","        \r\n","    return epoch_loss / len(iterator)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"K0AuUuKQko0J"},"source":["def evaluate(model, iterator, criterion):\r\n","    \r\n","    model.eval()\r\n","    \r\n","    epoch_loss = 0\r\n","    \r\n","    with torch.no_grad():\r\n","    \r\n","        for i, batch in enumerate(iterator):\r\n","\r\n","            src = batch.src\r\n","            trg = batch.trg\r\n","\r\n","            output, _ = model(src, trg[:,:-1])\r\n","        \r\n","            #output = [batch size, trg len - 1, output dim]\r\n","            #trg = [batch size, trg len]\r\n","\r\n","            output_dim = output.shape[-1]\r\n","            \r\n","            output = output.contiguous().view(-1, output_dim)\r\n","            trg = trg[:,1:].contiguous().view(-1)\r\n","\r\n","            #output = [batch size * trg len - 1, output dim]\r\n","            #trg = [batch size * trg len - 1]\r\n","            \r\n","            loss = criterion(output, trg)\r\n","\r\n","            epoch_loss += loss.item()\r\n","        \r\n","    return epoch_loss / len(iterator)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"fwMFMbGgkrWq"},"source":["def epoch_time(start_time, end_time):\r\n","    elapsed_time = end_time - start_time\r\n","    elapsed_mins = int(elapsed_time / 60)\r\n","    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\r\n","    return elapsed_mins, elapsed_secs"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TtCgxDm2kuDZ","outputId":"0c958764-6738-43af-df5c-59a98f9aae57"},"source":["N_EPOCHS = 10\r\n","CLIP = 0.1\r\n","\r\n","best_valid_loss = float('inf')\r\n","\r\n","for epoch in range(N_EPOCHS):\r\n","    \r\n","    start_time = time.time()\r\n","    \r\n","    train_loss = train(model, train_iterator, optimizer, criterion, CLIP)\r\n","    valid_loss = evaluate(model, valid_iterator, criterion)\r\n","    \r\n","    end_time = time.time()\r\n","    \r\n","    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\r\n","    \r\n","    if valid_loss < best_valid_loss:\r\n","        best_valid_loss = valid_loss\r\n","        torch.save(model.state_dict(), 'tut5-model.pt')\r\n","    \r\n","    print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\r\n","    print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):10.3f}')\r\n","    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. PPL: {math.exp(valid_loss):10.3f}')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Epoch: 01 | Time: 0m 15s\n","\tTrain Loss: 4.854 | Train PPL:    128.213\n","\t Val. Loss: 4.028 |  Val. PPL:     56.170\n","Epoch: 02 | Time: 0m 15s\n","\tTrain Loss: 4.194 | Train PPL:     66.288\n","\t Val. Loss: 3.792 |  Val. PPL:     44.336\n","Epoch: 03 | Time: 0m 15s\n","\tTrain Loss: 3.969 | Train PPL:     52.950\n","\t Val. Loss: 3.608 |  Val. PPL:     36.896\n","Epoch: 04 | Time: 0m 15s\n","\tTrain Loss: 3.804 | Train PPL:     44.882\n","\t Val. Loss: 3.509 |  Val. PPL:     33.428\n","Epoch: 05 | Time: 0m 15s\n","\tTrain Loss: 3.715 | Train PPL:     41.044\n","\t Val. Loss: 3.461 |  Val. PPL:     31.857\n","Epoch: 06 | Time: 0m 15s\n","\tTrain Loss: 3.681 | Train PPL:     39.667\n","\t Val. Loss: 3.513 |  Val. PPL:     33.550\n","Epoch: 07 | Time: 0m 15s\n","\tTrain Loss: 3.628 | Train PPL:     37.637\n","\t Val. Loss: 3.378 |  Val. PPL:     29.308\n","Epoch: 08 | Time: 0m 15s\n","\tTrain Loss: 3.561 | Train PPL:     35.197\n","\t Val. Loss: 3.364 |  Val. PPL:     28.918\n","Epoch: 09 | Time: 0m 15s\n","\tTrain Loss: 3.611 | Train PPL:     37.017\n","\t Val. Loss: 3.468 |  Val. PPL:     32.086\n","Epoch: 10 | Time: 0m 15s\n","\tTrain Loss: 4.012 | Train PPL:     55.274\n","\t Val. Loss: 17.629 |  Val. PPL: 45306364.382\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"mF8MwgNCfua6"},"source":["## Performance: Test Loss=3.370"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KkQolihQgB1A","outputId":"fbcb9b0f-9ebf-4b71-c05b-a0f02366d905"},"source":["model.load_state_dict(torch.load('tut5-model.pt'))\r\n","\r\n","test_loss = evaluate(model, test_iterator, criterion)\r\n","\r\n","print(f'| Test Loss: {test_loss:.3f} | Test PPL: {math.exp(test_loss):7.3f} |')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["| Test Loss: 3.370 | Test PPL:  29.069 |\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"9TAd7R6RfubB"},"source":["def translate_sentence(sentence, src_field, trg_field, model, device, max_len = 50):\r\n","\r\n","    model.eval()\r\n","        \r\n","    if isinstance(sentence, str):\r\n","        nlp = spacy.load('de')\r\n","        tokens = [token.text.lower() for token in nlp(sentence)]\r\n","    else:\r\n","        tokens = [token.lower() for token in sentence]\r\n","\r\n","    tokens = [src_field.init_token] + tokens + [src_field.eos_token]\r\n","        \r\n","    src_indexes = [src_field.vocab.stoi[token] for token in tokens]\r\n","\r\n","    src_tensor = torch.LongTensor(src_indexes).unsqueeze(0).to(device)\r\n","\r\n","    with torch.no_grad():\r\n","        encoder_conved, encoder_combined = model.encoder(src_tensor)\r\n","\r\n","    trg_indexes = [trg_field.vocab.stoi[trg_field.init_token]]\r\n","\r\n","    for i in range(max_len):\r\n","\r\n","        trg_tensor = torch.LongTensor(trg_indexes).unsqueeze(0).to(device)\r\n","\r\n","        with torch.no_grad():\r\n","            output, attention = model.decoder(trg_tensor, encoder_conved, encoder_combined)\r\n","        \r\n","        pred_token = output.argmax(2)[:,-1].item()\r\n","        \r\n","        trg_indexes.append(pred_token)\r\n","\r\n","        if pred_token == trg_field.vocab.stoi[trg_field.eos_token]:\r\n","            break\r\n","    \r\n","    trg_tokens = [trg_field.vocab.itos[i] for i in trg_indexes]\r\n","    \r\n","    return trg_tokens[1:], attention"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"IaaJpL7VfubC"},"source":["def display_attention(sentence, translation, attention):\r\n","    \r\n","    fig = plt.figure(figsize=(10,10))\r\n","    ax = fig.add_subplot(111)\r\n","        \r\n","    attention = attention.squeeze(0).cpu().detach().numpy()\r\n","    \r\n","    cax = ax.matshow(attention, cmap='bone')\r\n","   \r\n","    ax.tick_params(labelsize=15)\r\n","    ax.set_xticklabels(['']+['<sos>']+[t.lower() for t in sentence]+['<eos>'], \r\n","                       rotation=45)\r\n","    ax.set_yticklabels(['']+translation)\r\n","\r\n","    ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\r\n","    ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\r\n","\r\n","    plt.show()\r\n","    plt.close()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vMhZSCbtfubC","outputId":"72e2a287-a679-43a1-a9d6-618bb5e43b92"},"source":["example_idx = 2\r\n","\r\n","src = vars(train_data.examples[example_idx])['src']\r\n","trg = vars(train_data.examples[example_idx])['trg']\r\n","\r\n","print(f'src = {src}')\r\n","print(f'trg = {trg}')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["src = ['he', 'is', 'mean', '.']\n","trg = ['他', '很', '凶', '。']\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"YJ1C2oYZfubD","outputId":"053d31da-8c51-4427-ba58-b5ce3de34036"},"source":["example_idx = 6\r\n","\r\n","src = vars(valid_data.examples[example_idx])['src']\r\n","trg = vars(valid_data.examples[example_idx])['trg']\r\n","\r\n","print(f'src = {src}')\r\n","print(f'trg = {trg}')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["src = ['what', \"'s\", 'your', 'home', 'address', '?']\n","trg = ['你家', '的', '地址', '是', '什麼', '？']\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7Dh9GPkVfubD","outputId":"3d73c370-5abf-4637-afd1-28348600148a"},"source":["example_idx = 10\r\n","\r\n","src_example = vars(test_data.examples[example_idx])['src']\r\n","trg_example = vars(test_data.examples[example_idx])['trg']\r\n","\r\n","print(f'src = {src_example}')\r\n","print(f'trg = {trg_example}')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["src = ['i', 'hope', 'tom', 'appreciates', 'it', '.']\n","trg = ['我', '希望', '汤姆', '欣赏', '它', '。']\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"piVNso6_fubD","outputId":"ed31cb52-eb5b-46c9-a683-19ab60133f8b"},"source":["translation, attention = translate_sentence(src_example, SRC, TRG, model, device)\r\n","\r\n","print(f'predicted trg = {translation}')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["predicted trg = ['我', '希望', '汤姆', '<unk>', '。', '<eos>']\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"sk4POL4jfubE"},"source":["from torchtext.data.metrics import bleu_score\r\n","\r\n","def calculate_bleu(data, src_field, trg_field, model, device, max_len = 50):\r\n","    \r\n","    trgs = []\r\n","    pred_trgs = []\r\n","    \r\n","    for datum in data:\r\n","        \r\n","        src = vars(datum)['src']\r\n","        trg = vars(datum)['trg']\r\n","        \r\n","        pred_trg, _ = translate_sentence(src, src_field, trg_field, model, device, max_len)\r\n","        \r\n","        #cut off <eos> token\r\n","        pred_trg = pred_trg[:-1]\r\n","        \r\n","        pred_trgs.append(pred_trg)\r\n","        trgs.append([trg])\r\n","        \r\n","    return bleu_score(pred_trgs, trgs)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4tr-XWC-fubE","outputId":"8b4875b9-3d6a-4aca-f32c-3427edb4cca0"},"source":["bleu_score = calculate_bleu(test_data, SRC, TRG, model, device)\r\n","\r\n","print(f'BLEU score = {bleu_score*100:.2f}')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["BLEU score = 3.64\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"Jhp6xsm_sLiQ"},"source":["#Tutorial 6"]},{"cell_type":"code","metadata":{"id":"d5X1yzubsLiR"},"source":["def tokenize_zh(text):\r\n","    \"\"\"\r\n","    Tokenizes Chinese text from a string into a list of strings (tokens) and reverses it\r\n","    \"\"\"\r\n","    return [tok.text for tok in spacy_zh.tokenizer(text)]#[::-1]\r\n","\r\n","def tokenize_en(text):\r\n","    \"\"\"\r\n","    Tokenizes English text from a string into a list of strings (tokens)\r\n","    \"\"\"\r\n","    return [tok.text for tok in spacy_en.tokenizer(text)]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nzgZNKZ1QI1N","outputId":"4381dc50-9ac6-48f9-db6e-03d3d4155ad8"},"source":["tokenize_zh"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<function __main__.tokenize_zh>"]},"metadata":{"tags":[]},"execution_count":174}]},{"cell_type":"code","metadata":{"id":"KcoBxcWPsLiR"},"source":["SRC = Field(tokenize = tokenize_en, \r\n","            init_token = '<sos>', \r\n","            eos_token = '<eos>', \r\n","            lower = True,\r\n","            batch_first = True)\r\n","\r\n","TRG = Field(tokenize = tokenize_zh, \r\n","            init_token = '<sos>', \r\n","            eos_token = '<eos>', \r\n","            lower = True,\r\n","            batch_first = True)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_rgsFnTQsLiS"},"source":["import torchtext\r\n","\r\n","dataset = torchtext.data.TabularDataset(\r\n","    path='/content/drive/MyDrive/NLP/pj2/project.csv',\r\n","    format='csv',\r\n","    #skip_header=True,\r\n","    fields=[('src', SRC), ('trg', TRG)]\r\n",")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"iIB1G6SusLiS"},"source":["train_data, valid_data, test_data = dataset.split(split_ratio=[0.6, 0.2, 0.2])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"An7DgUO7sLiS","outputId":"68d79a06-8b30-4801-9710-55f39624d0e1"},"source":["print(f\"Number of training examples: {len(train_data.examples)}\")\r\n","print(f\"Number of validation examples: {len(valid_data.examples)}\")\r\n","print(f\"Number of testing examples: {len(test_data.examples)}\")"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Number of training examples: 14416\n","Number of validation examples: 4806\n","Number of testing examples: 4805\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XQO6sQxLsLiT","outputId":"c8375b61-fd22-4972-e9dc-6fc33ab56cfa"},"source":["print(vars(train_data.examples[0]))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["{'src': ['the', 'bank', 'was', 'held', 'up', 'a', 'week', 'ago', '.'], 'trg': ['這家', '銀行', '一', '週', '前', '被', '搶劫', '。']}\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"T3ntA9tFsLiT"},"source":["SRC.build_vocab(train_data, min_freq = 2)\r\n","TRG.build_vocab(train_data, min_freq = 2)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"I5-Tpmn8sLiT","outputId":"8c30aee1-bf84-4c62-affb-458aa7239217"},"source":["print(f\"Unique tokens in source (zh) vocabulary: {len(SRC.vocab)}\")\r\n","print(f\"Unique tokens in target (en) vocabulary: {len(TRG.vocab)}\")"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Unique tokens in source (zh) vocabulary: 3055\n","Unique tokens in target (en) vocabulary: 4494\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"I7g_rax8sLiT"},"source":["device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"A6B2ZJussLiT"},"source":["BATCH_SIZE = 128\r\n","\r\n","train_iterator, valid_iterator, test_iterator = BucketIterator.splits(\r\n","    (train_data, valid_data, test_data),\r\n","     sort=False, \r\n","     batch_size = BATCH_SIZE,\r\n","     device = device)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"2sr4JPJ5s7hi"},"source":["class Encoder(nn.Module):\r\n","    def __init__(self, \r\n","                 input_dim, \r\n","                 hid_dim, \r\n","                 n_layers, \r\n","                 n_heads, \r\n","                 pf_dim,\r\n","                 dropout, \r\n","                 device,\r\n","                 max_length = 100):\r\n","        super().__init__()\r\n","\r\n","        self.device = device\r\n","        \r\n","        self.tok_embedding = nn.Embedding(input_dim, hid_dim)\r\n","        self.pos_embedding = nn.Embedding(max_length, hid_dim)\r\n","        \r\n","        self.layers = nn.ModuleList([EncoderLayer(hid_dim, \r\n","                                                  n_heads, \r\n","                                                  pf_dim,\r\n","                                                  dropout, \r\n","                                                  device) \r\n","                                     for _ in range(n_layers)])\r\n","        \r\n","        self.dropout = nn.Dropout(dropout)\r\n","        \r\n","        self.scale = torch.sqrt(torch.FloatTensor([hid_dim])).to(device)\r\n","        \r\n","    def forward(self, src, src_mask):\r\n","        \r\n","        #src = [batch size, src len]\r\n","        #src_mask = [batch size, 1, 1, src len]\r\n","        \r\n","        batch_size = src.shape[0]\r\n","        src_len = src.shape[1]\r\n","        \r\n","        pos = torch.arange(0, src_len).unsqueeze(0).repeat(batch_size, 1).to(self.device)\r\n","        \r\n","        #pos = [batch size, src len]\r\n","        \r\n","        src = self.dropout((self.tok_embedding(src) * self.scale) + self.pos_embedding(pos))\r\n","        \r\n","        #src = [batch size, src len, hid dim]\r\n","        \r\n","        for layer in self.layers:\r\n","            src = layer(src, src_mask)\r\n","            \r\n","        #src = [batch size, src len, hid dim]\r\n","            \r\n","        return src"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"5zyTledatTmc"},"source":["class EncoderLayer(nn.Module):\r\n","    def __init__(self, \r\n","                 hid_dim, \r\n","                 n_heads, \r\n","                 pf_dim,  \r\n","                 dropout, \r\n","                 device):\r\n","        super().__init__()\r\n","        \r\n","        self.self_attn_layer_norm = nn.LayerNorm(hid_dim)\r\n","        self.ff_layer_norm = nn.LayerNorm(hid_dim)\r\n","        self.self_attention = MultiHeadAttentionLayer(hid_dim, n_heads, dropout, device)\r\n","        self.positionwise_feedforward = PositionwiseFeedforwardLayer(hid_dim, \r\n","                                                                     pf_dim, \r\n","                                                                     dropout)\r\n","        self.dropout = nn.Dropout(dropout)\r\n","        \r\n","    def forward(self, src, src_mask):\r\n","        \r\n","        #src = [batch size, src len, hid dim]\r\n","        #src_mask = [batch size, 1, 1, src len] \r\n","                \r\n","        #self attention\r\n","        _src, _ = self.self_attention(src, src, src, src_mask)\r\n","        \r\n","        #dropout, residual connection and layer norm\r\n","        src = self.self_attn_layer_norm(src + self.dropout(_src))\r\n","        \r\n","        #src = [batch size, src len, hid dim]\r\n","        \r\n","        #positionwise feedforward\r\n","        _src = self.positionwise_feedforward(src)\r\n","        \r\n","        #dropout, residual and layer norm\r\n","        src = self.ff_layer_norm(src + self.dropout(_src))\r\n","        \r\n","        #src = [batch size, src len, hid dim]\r\n","        \r\n","        return src"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"cLUxl31LtY-A"},"source":["class MultiHeadAttentionLayer(nn.Module):\r\n","    def __init__(self, hid_dim, n_heads, dropout, device):\r\n","        super().__init__()\r\n","        \r\n","        assert hid_dim % n_heads == 0\r\n","        \r\n","        self.hid_dim = hid_dim\r\n","        self.n_heads = n_heads\r\n","        self.head_dim = hid_dim // n_heads\r\n","        \r\n","        self.fc_q = nn.Linear(hid_dim, hid_dim)\r\n","        self.fc_k = nn.Linear(hid_dim, hid_dim)\r\n","        self.fc_v = nn.Linear(hid_dim, hid_dim)\r\n","        \r\n","        self.fc_o = nn.Linear(hid_dim, hid_dim)\r\n","        \r\n","        self.dropout = nn.Dropout(dropout)\r\n","        \r\n","        self.scale = torch.sqrt(torch.FloatTensor([self.head_dim])).to(device)\r\n","        \r\n","    def forward(self, query, key, value, mask = None):\r\n","        \r\n","        batch_size = query.shape[0]\r\n","        \r\n","        #query = [batch size, query len, hid dim]\r\n","        #key = [batch size, key len, hid dim]\r\n","        #value = [batch size, value len, hid dim]\r\n","                \r\n","        Q = self.fc_q(query)\r\n","        K = self.fc_k(key)\r\n","        V = self.fc_v(value)\r\n","        \r\n","        #Q = [batch size, query len, hid dim]\r\n","        #K = [batch size, key len, hid dim]\r\n","        #V = [batch size, value len, hid dim]\r\n","                \r\n","        Q = Q.view(batch_size, -1, self.n_heads, self.head_dim).permute(0, 2, 1, 3)\r\n","        K = K.view(batch_size, -1, self.n_heads, self.head_dim).permute(0, 2, 1, 3)\r\n","        V = V.view(batch_size, -1, self.n_heads, self.head_dim).permute(0, 2, 1, 3)\r\n","        \r\n","        #Q = [batch size, n heads, query len, head dim]\r\n","        #K = [batch size, n heads, key len, head dim]\r\n","        #V = [batch size, n heads, value len, head dim]\r\n","                \r\n","        energy = torch.matmul(Q, K.permute(0, 1, 3, 2)) / self.scale\r\n","        \r\n","        #energy = [batch size, n heads, query len, key len]\r\n","        \r\n","        if mask is not None:\r\n","            energy = energy.masked_fill(mask == 0, -1e10)\r\n","        \r\n","        attention = torch.softmax(energy, dim = -1)\r\n","                \r\n","        #attention = [batch size, n heads, query len, key len]\r\n","                \r\n","        x = torch.matmul(self.dropout(attention), V)\r\n","        \r\n","        #x = [batch size, n heads, query len, head dim]\r\n","        \r\n","        x = x.permute(0, 2, 1, 3).contiguous()\r\n","        \r\n","        #x = [batch size, query len, n heads, head dim]\r\n","        \r\n","        x = x.view(batch_size, -1, self.hid_dim)\r\n","        \r\n","        #x = [batch size, query len, hid dim]\r\n","        \r\n","        x = self.fc_o(x)\r\n","        \r\n","        #x = [batch size, query len, hid dim]\r\n","        \r\n","        return x, attention"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ncIZ5Hl-t6C0"},"source":["class PositionwiseFeedforwardLayer(nn.Module):\r\n","    def __init__(self, hid_dim, pf_dim, dropout):\r\n","        super().__init__()\r\n","        \r\n","        self.fc_1 = nn.Linear(hid_dim, pf_dim)\r\n","        self.fc_2 = nn.Linear(pf_dim, hid_dim)\r\n","        \r\n","        self.dropout = nn.Dropout(dropout)\r\n","        \r\n","    def forward(self, x):\r\n","        \r\n","        #x = [batch size, seq len, hid dim]\r\n","        \r\n","        x = self.dropout(torch.relu(self.fc_1(x)))\r\n","        \r\n","        #x = [batch size, seq len, pf dim]\r\n","        \r\n","        x = self.fc_2(x)\r\n","        \r\n","        #x = [batch size, seq len, hid dim]\r\n","        \r\n","        return x"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"TX9fFpNdt906"},"source":["class Decoder(nn.Module):\r\n","    def __init__(self, \r\n","                 output_dim, \r\n","                 hid_dim, \r\n","                 n_layers, \r\n","                 n_heads, \r\n","                 pf_dim, \r\n","                 dropout, \r\n","                 device,\r\n","                 max_length = 100):\r\n","        super().__init__()\r\n","        \r\n","        self.device = device\r\n","        \r\n","        self.tok_embedding = nn.Embedding(output_dim, hid_dim)\r\n","        self.pos_embedding = nn.Embedding(max_length, hid_dim)\r\n","        \r\n","        self.layers = nn.ModuleList([DecoderLayer(hid_dim, \r\n","                                                  n_heads, \r\n","                                                  pf_dim, \r\n","                                                  dropout, \r\n","                                                  device)\r\n","                                     for _ in range(n_layers)])\r\n","        \r\n","        self.fc_out = nn.Linear(hid_dim, output_dim)\r\n","        \r\n","        self.dropout = nn.Dropout(dropout)\r\n","        \r\n","        self.scale = torch.sqrt(torch.FloatTensor([hid_dim])).to(device)\r\n","        \r\n","    def forward(self, trg, enc_src, trg_mask, src_mask):\r\n","        \r\n","        #trg = [batch size, trg len]\r\n","        #enc_src = [batch size, src len, hid dim]\r\n","        #trg_mask = [batch size, 1, trg len, trg len]\r\n","        #src_mask = [batch size, 1, 1, src len]\r\n","                \r\n","        batch_size = trg.shape[0]\r\n","        trg_len = trg.shape[1]\r\n","        \r\n","        pos = torch.arange(0, trg_len).unsqueeze(0).repeat(batch_size, 1).to(self.device)\r\n","                            \r\n","        #pos = [batch size, trg len]\r\n","            \r\n","        trg = self.dropout((self.tok_embedding(trg) * self.scale) + self.pos_embedding(pos))\r\n","                \r\n","        #trg = [batch size, trg len, hid dim]\r\n","        \r\n","        for layer in self.layers:\r\n","            trg, attention = layer(trg, enc_src, trg_mask, src_mask)\r\n","        \r\n","        #trg = [batch size, trg len, hid dim]\r\n","        #attention = [batch size, n heads, trg len, src len]\r\n","        \r\n","        output = self.fc_out(trg)\r\n","        \r\n","        #output = [batch size, trg len, output dim]\r\n","            \r\n","        return output, attention"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"1AO8ihxSuC5X"},"source":["class DecoderLayer(nn.Module):\r\n","    def __init__(self, \r\n","                 hid_dim, \r\n","                 n_heads, \r\n","                 pf_dim, \r\n","                 dropout, \r\n","                 device):\r\n","        super().__init__()\r\n","        \r\n","        self.self_attn_layer_norm = nn.LayerNorm(hid_dim)\r\n","        self.enc_attn_layer_norm = nn.LayerNorm(hid_dim)\r\n","        self.ff_layer_norm = nn.LayerNorm(hid_dim)\r\n","        self.self_attention = MultiHeadAttentionLayer(hid_dim, n_heads, dropout, device)\r\n","        self.encoder_attention = MultiHeadAttentionLayer(hid_dim, n_heads, dropout, device)\r\n","        self.positionwise_feedforward = PositionwiseFeedforwardLayer(hid_dim, \r\n","                                                                     pf_dim, \r\n","                                                                     dropout)\r\n","        self.dropout = nn.Dropout(dropout)\r\n","        \r\n","    def forward(self, trg, enc_src, trg_mask, src_mask):\r\n","        \r\n","        #trg = [batch size, trg len, hid dim]\r\n","        #enc_src = [batch size, src len, hid dim]\r\n","        #trg_mask = [batch size, 1, trg len, trg len]\r\n","        #src_mask = [batch size, 1, 1, src len]\r\n","        \r\n","        #self attention\r\n","        _trg, _ = self.self_attention(trg, trg, trg, trg_mask)\r\n","        \r\n","        #dropout, residual connection and layer norm\r\n","        trg = self.self_attn_layer_norm(trg + self.dropout(_trg))\r\n","            \r\n","        #trg = [batch size, trg len, hid dim]\r\n","            \r\n","        #encoder attention\r\n","        _trg, attention = self.encoder_attention(trg, enc_src, enc_src, src_mask)\r\n","        \r\n","        #dropout, residual connection and layer norm\r\n","        trg = self.enc_attn_layer_norm(trg + self.dropout(_trg))\r\n","                    \r\n","        #trg = [batch size, trg len, hid dim]\r\n","        \r\n","        #positionwise feedforward\r\n","        _trg = self.positionwise_feedforward(trg)\r\n","        \r\n","        #dropout, residual and layer norm\r\n","        trg = self.ff_layer_norm(trg + self.dropout(_trg))\r\n","        \r\n","        #trg = [batch size, trg len, hid dim]\r\n","        #attention = [batch size, n heads, trg len, src len]\r\n","        \r\n","        return trg, attention"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"8GcL982UudQ-"},"source":["class Seq2Seq(nn.Module):\r\n","    def __init__(self, \r\n","                 encoder, \r\n","                 decoder, \r\n","                 src_pad_idx, \r\n","                 trg_pad_idx, \r\n","                 device):\r\n","        super().__init__()\r\n","        \r\n","        self.encoder = encoder\r\n","        self.decoder = decoder\r\n","        self.src_pad_idx = src_pad_idx\r\n","        self.trg_pad_idx = trg_pad_idx\r\n","        self.device = device\r\n","        \r\n","    def make_src_mask(self, src):\r\n","        \r\n","        #src = [batch size, src len]\r\n","        \r\n","        src_mask = (src != self.src_pad_idx).unsqueeze(1).unsqueeze(2)\r\n","\r\n","        #src_mask = [batch size, 1, 1, src len]\r\n","\r\n","        return src_mask\r\n","    \r\n","    def make_trg_mask(self, trg):\r\n","        \r\n","        #trg = [batch size, trg len]\r\n","        \r\n","        trg_pad_mask = (trg != self.trg_pad_idx).unsqueeze(1).unsqueeze(2)\r\n","        \r\n","        #trg_pad_mask = [batch size, 1, 1, trg len]\r\n","        \r\n","        trg_len = trg.shape[1]\r\n","        \r\n","        trg_sub_mask = torch.tril(torch.ones((trg_len, trg_len), device = self.device)).bool()\r\n","        \r\n","        #trg_sub_mask = [trg len, trg len]\r\n","            \r\n","        trg_mask = trg_pad_mask & trg_sub_mask\r\n","        \r\n","        #trg_mask = [batch size, 1, trg len, trg len]\r\n","        \r\n","        return trg_mask\r\n","\r\n","    def forward(self, src, trg):\r\n","        \r\n","        #src = [batch size, src len]\r\n","        #trg = [batch size, trg len]\r\n","                \r\n","        src_mask = self.make_src_mask(src)\r\n","        trg_mask = self.make_trg_mask(trg)\r\n","        \r\n","        #src_mask = [batch size, 1, 1, src len]\r\n","        #trg_mask = [batch size, 1, trg len, trg len]\r\n","        \r\n","        enc_src = self.encoder(src, src_mask)\r\n","        \r\n","        #enc_src = [batch size, src len, hid dim]\r\n","                \r\n","        output, attention = self.decoder(trg, enc_src, trg_mask, src_mask)\r\n","        \r\n","        #output = [batch size, trg len, output dim]\r\n","        #attention = [batch size, n heads, trg len, src len]\r\n","        \r\n","        return output, attention"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"NjPk7dowuiVy"},"source":["INPUT_DIM = len(SRC.vocab)\r\n","OUTPUT_DIM = len(TRG.vocab)\r\n","HID_DIM = 256\r\n","ENC_LAYERS = 3\r\n","DEC_LAYERS = 3\r\n","ENC_HEADS = 8\r\n","DEC_HEADS = 8\r\n","ENC_PF_DIM = 512\r\n","DEC_PF_DIM = 512\r\n","ENC_DROPOUT = 0.1\r\n","DEC_DROPOUT = 0.1\r\n","\r\n","enc = Encoder(INPUT_DIM, \r\n","              HID_DIM, \r\n","              ENC_LAYERS, \r\n","              ENC_HEADS, \r\n","              ENC_PF_DIM, \r\n","              ENC_DROPOUT, \r\n","              device)\r\n","\r\n","dec = Decoder(OUTPUT_DIM, \r\n","              HID_DIM, \r\n","              DEC_LAYERS, \r\n","              DEC_HEADS, \r\n","              DEC_PF_DIM, \r\n","              DEC_DROPOUT, \r\n","              device)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"-wENPwPTulQy"},"source":["SRC_PAD_IDX = SRC.vocab.stoi[SRC.pad_token]\r\n","TRG_PAD_IDX = TRG.vocab.stoi[TRG.pad_token]\r\n","\r\n","model = Seq2Seq(enc, dec, SRC_PAD_IDX, TRG_PAD_IDX, device).to(device)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"He4EJp4NuqZZ","outputId":"6aab8bc1-8345-4fdf-8240-4fdfdfac5dc4"},"source":["def count_parameters(model):\r\n","    return sum(p.numel() for p in model.parameters() if p.requires_grad)\r\n","\r\n","print(f'The model has {count_parameters(model):,} trainable parameters')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["The model has 7,092,366 trainable parameters\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"6LaQYFl-usSe"},"source":["def initialize_weights(m):\r\n","    if hasattr(m, 'weight') and m.weight.dim() > 1:\r\n","        nn.init.xavier_uniform_(m.weight.data)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"o91zqnEyuuMF"},"source":["model.apply(initialize_weights);"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZA8vyW8buwFP"},"source":["LEARNING_RATE = 0.0005\r\n","\r\n","optimizer = torch.optim.Adam(model.parameters(), lr = LEARNING_RATE)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"1OAXVulDuxkd"},"source":["criterion = nn.CrossEntropyLoss(ignore_index = TRG_PAD_IDX)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"FXoTvhLuu1vP"},"source":["def train(model, iterator, optimizer, criterion, clip):\r\n","    \r\n","    model.train()\r\n","    \r\n","    epoch_loss = 0\r\n","    \r\n","    for i, batch in enumerate(iterator):\r\n","        \r\n","        src = batch.src\r\n","        trg = batch.trg\r\n","        \r\n","        optimizer.zero_grad()\r\n","        \r\n","        output, _ = model(src, trg[:,:-1])\r\n","                \r\n","        #output = [batch size, trg len - 1, output dim]\r\n","        #trg = [batch size, trg len]\r\n","            \r\n","        output_dim = output.shape[-1]\r\n","            \r\n","        output = output.contiguous().view(-1, output_dim)\r\n","        trg = trg[:,1:].contiguous().view(-1)\r\n","                \r\n","        #output = [batch size * trg len - 1, output dim]\r\n","        #trg = [batch size * trg len - 1]\r\n","            \r\n","        loss = criterion(output, trg)\r\n","        \r\n","        loss.backward()\r\n","        \r\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\r\n","        \r\n","        optimizer.step()\r\n","        \r\n","        epoch_loss += loss.item()\r\n","        \r\n","    return epoch_loss / len(iterator)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"pDZsUSRxu5_7"},"source":["def evaluate(model, iterator, criterion):\r\n","    \r\n","    model.eval()\r\n","    \r\n","    epoch_loss = 0\r\n","    \r\n","    with torch.no_grad():\r\n","    \r\n","        for i, batch in enumerate(iterator):\r\n","\r\n","            src = batch.src\r\n","            trg = batch.trg\r\n","\r\n","            output, _ = model(src, trg[:,:-1])\r\n","            \r\n","            #output = [batch size, trg len - 1, output dim]\r\n","            #trg = [batch size, trg len]\r\n","            \r\n","            output_dim = output.shape[-1]\r\n","            \r\n","            output = output.contiguous().view(-1, output_dim)\r\n","            trg = trg[:,1:].contiguous().view(-1)\r\n","            \r\n","            #output = [batch size * trg len - 1, output dim]\r\n","            #trg = [batch size * trg len - 1]\r\n","            \r\n","            loss = criterion(output, trg)\r\n","\r\n","            epoch_loss += loss.item()\r\n","        \r\n","    return epoch_loss / len(iterator)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"cDAbdbrsu8fy"},"source":["def epoch_time(start_time, end_time):\r\n","    elapsed_time = end_time - start_time\r\n","    elapsed_mins = int(elapsed_time / 60)\r\n","    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\r\n","    return elapsed_mins, elapsed_secs"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"exKqsggqu_ah","outputId":"049222a1-f4a5-4d3e-b445-85de5c40e84e"},"source":["N_EPOCHS = 10\r\n","CLIP = 1\r\n","\r\n","best_valid_loss = float('inf')\r\n","\r\n","for epoch in range(N_EPOCHS):\r\n","    \r\n","    start_time = time.time()\r\n","    \r\n","    train_loss = train(model, train_iterator, optimizer, criterion, CLIP)\r\n","    valid_loss = evaluate(model, valid_iterator, criterion)\r\n","    \r\n","    end_time = time.time()\r\n","    \r\n","    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\r\n","    \r\n","    if valid_loss < best_valid_loss:\r\n","        best_valid_loss = valid_loss\r\n","        torch.save(model.state_dict(), 'tut6-model.pt')\r\n","    \r\n","    print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\r\n","    print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}')\r\n","    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. PPL: {math.exp(valid_loss):7.3f}')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Epoch: 01 | Time: 0m 9s\n","\tTrain Loss: 4.919 | Train PPL: 136.801\n","\t Val. Loss: 3.822 |  Val. PPL:  45.716\n","Epoch: 02 | Time: 0m 9s\n","\tTrain Loss: 3.781 | Train PPL:  43.859\n","\t Val. Loss: 3.377 |  Val. PPL:  29.297\n","Epoch: 03 | Time: 0m 9s\n","\tTrain Loss: 3.289 | Train PPL:  26.817\n","\t Val. Loss: 3.095 |  Val. PPL:  22.080\n","Epoch: 04 | Time: 0m 9s\n","\tTrain Loss: 2.904 | Train PPL:  18.238\n","\t Val. Loss: 2.869 |  Val. PPL:  17.619\n","Epoch: 05 | Time: 0m 9s\n","\tTrain Loss: 2.585 | Train PPL:  13.266\n","\t Val. Loss: 2.725 |  Val. PPL:  15.254\n","Epoch: 06 | Time: 0m 9s\n","\tTrain Loss: 2.299 | Train PPL:   9.964\n","\t Val. Loss: 2.590 |  Val. PPL:  13.328\n","Epoch: 07 | Time: 0m 9s\n","\tTrain Loss: 2.047 | Train PPL:   7.743\n","\t Val. Loss: 2.494 |  Val. PPL:  12.112\n","Epoch: 08 | Time: 0m 9s\n","\tTrain Loss: 1.821 | Train PPL:   6.177\n","\t Val. Loss: 2.454 |  Val. PPL:  11.635\n","Epoch: 09 | Time: 0m 9s\n","\tTrain Loss: 1.618 | Train PPL:   5.045\n","\t Val. Loss: 2.401 |  Val. PPL:  11.030\n","Epoch: 10 | Time: 0m 9s\n","\tTrain Loss: 1.437 | Train PPL:   4.209\n","\t Val. Loss: 2.392 |  Val. PPL:  10.931\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Eqi1byGZxs7P","outputId":"24f4edba-5b6b-42cc-f6c7-ff94706dc48d"},"source":["model.load_state_dict(torch.load('tut6-model.pt'))\r\n","\r\n","test_loss = evaluate(model, test_iterator, criterion)\r\n","\r\n","print(f'| Test Loss: {test_loss:.3f} | Test PPL: {math.exp(test_loss):7.3f} |')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["| Test Loss: 2.413 | Test PPL:  11.168 |\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"ehF69XWOwbj9"},"source":["## Performance: Test Loss=2.413"]},{"cell_type":"code","metadata":{"id":"TgICEKYywa4S"},"source":["def translate_sentence(sentence, src_field, trg_field, model, device, max_len = 50):\r\n","    \r\n","    model.eval()\r\n","        \r\n","    if isinstance(sentence, str):\r\n","        nlp = spacy.load('en')\r\n","        tokens = [token.text.lower() for token in nlp(sentence)]\r\n","    else:\r\n","        tokens = [token.lower() for token in sentence]\r\n","\r\n","    tokens = [src_field.init_token] + tokens + [src_field.eos_token]\r\n","        \r\n","    src_indexes = [src_field.vocab.stoi[token] for token in tokens]\r\n","\r\n","    src_tensor = torch.LongTensor(src_indexes).unsqueeze(0).to(device)\r\n","    \r\n","    src_mask = model.make_src_mask(src_tensor)\r\n","    \r\n","    with torch.no_grad():\r\n","        enc_src = model.encoder(src_tensor, src_mask)\r\n","\r\n","    trg_indexes = [trg_field.vocab.stoi[trg_field.init_token]]\r\n","\r\n","    for i in range(max_len):\r\n","\r\n","        trg_tensor = torch.LongTensor(trg_indexes).unsqueeze(0).to(device)\r\n","\r\n","        trg_mask = model.make_trg_mask(trg_tensor)\r\n","        \r\n","        with torch.no_grad():\r\n","            output, attention = model.decoder(trg_tensor, enc_src, trg_mask, src_mask)\r\n","        \r\n","        pred_token = output.argmax(2)[:,-1].item()\r\n","        \r\n","        trg_indexes.append(pred_token)\r\n","\r\n","        if pred_token == trg_field.vocab.stoi[trg_field.eos_token]:\r\n","            break\r\n","    \r\n","    trg_tokens = [trg_field.vocab.itos[i] for i in trg_indexes]\r\n","    \r\n","    return trg_tokens, attention #[1:]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"tqXYa2L1x8aQ"},"source":["def display_attention(sentence, translation, attention, n_heads = 8, n_rows = 4, n_cols = 2):\r\n","    \r\n","    assert n_rows * n_cols == n_heads\r\n","    \r\n","    fig = plt(fontproperties=zhfont).figure(figsize=(15,25))\r\n","    \r\n","    for i in range(n_heads):\r\n","        \r\n","        ax = fig.add_subplot(n_rows, n_cols, i+1)\r\n","        \r\n","        _attention = attention.squeeze(0)[i].cpu().detach().numpy()\r\n","\r\n","        cax = ax.matshow(_attention, cmap='bone')\r\n","\r\n","        ax.tick_params(labelsize=12)\r\n","        ax.set_xticklabels(['']+['<sos>']+[t.lower() for t in sentence]+['<eos>'], \r\n","                           rotation=45)\r\n","        ax.set_yticklabels(['']+translation)\r\n","\r\n","        ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\r\n","        ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\r\n","\r\n","    plt.show()\r\n","    plt.close()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"RvrTyluQx-7c","outputId":"292da1bf-d6a3-457c-c400-9fcbd0240ffe"},"source":["example_idx = 8\r\n","\r\n","src = vars(train_data.examples[example_idx])['src']\r\n","trg = vars(train_data.examples[example_idx])['trg']\r\n","\r\n","print(f'src = {src}')\r\n","print(f'trg = {trg}')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["src = ['i', 'ca', \"n't\", 'drink', 'milk', '.']\n","trg = ['我', '不', '能', '喝', '牛奶', '。']\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"PjEY9BiFydNV","outputId":"8c88aef7-b008-4c8d-e069-e9277dde31bf"},"source":["example_idx = 6\r\n","\r\n","src = vars(valid_data.examples[example_idx])['src']\r\n","trg = vars(valid_data.examples[example_idx])['trg']\r\n","\r\n","print(f'src = {src}')\r\n","print(f'trg = {trg}')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["src = ['tom', 'told', 'me', 'where', 'he', 'lived', '.']\n","trg = ['湯姆', '告訴', '我', '他', '住在', '哪裡', '。']\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jB3sCkXbyij_","outputId":"877ea1bf-5cfd-4bc2-e56a-ff2566eccaa4"},"source":["example_idx = 20\r\n","\r\n","src_example = vars(test_data.examples[example_idx])['src']\r\n","trg_example = vars(test_data.examples[example_idx])['trg']\r\n","\r\n","print(f'src = {src_example}')\r\n","print(f'trg = {trg_example}')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["src = ['we', 'get', 'the', 'materials', 'from', 'malaysia', '.']\n","trg = ['我', '們從', '馬來', '西亞', '得到', '材料', '。']\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4RIuXC2v3_DF","outputId":"6ae329f0-e7b3-4b08-f0af-f5b7aef8f91c"},"source":["translation, attention = translate_sentence(src_example, SRC, TRG, model, device)\r\n","\r\n","print(f'predicted trg = {translation}')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["predicted trg = ['<sos>', '我们', '从', '<unk>', '到', '達', '了', '<unk>', '。', '<eos>']\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"PiQatbxjynQV"},"source":["from torchtext.data.metrics import bleu_score\r\n","\r\n","def calculate_bleu(data, src_field, trg_field, model, device, max_len = 50):\r\n","    \r\n","    trgs = []\r\n","    pred_trgs = []\r\n","    \r\n","    for datum in data:\r\n","        \r\n","        src = vars(datum)['src']\r\n","        trg = vars(datum)['trg']\r\n","        \r\n","        pred_trg, _ = translate_sentence(src, src_field, trg_field, model, device, max_len)\r\n","        \r\n","        #cut off <eos> token\r\n","        pred_trg = pred_trg[:-1]\r\n","        \r\n","        pred_trgs.append(pred_trg)\r\n","        trgs.append([trg])\r\n","        \r\n","    return bleu_score(pred_trgs, trgs)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dpKEHXCl2d-h","outputId":"639f30b9-aee6-4f52-99fe-5e84b0f95843"},"source":["bleu_score = calculate_bleu(test_data, SRC, TRG, model, device)\r\n","\r\n","print(f'BLEU score = {bleu_score*100:.2f}')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["BLEU score = 11.83\n"],"name":"stdout"}]}]}